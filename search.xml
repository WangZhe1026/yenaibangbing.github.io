<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>（摆烂版）Largrange Dual Problem</title>
      <link href="/2022/10/25/%EF%BC%88%E6%91%86%E7%83%82%E7%89%88%EF%BC%89Largrange-Dual-Problem/"/>
      <url>/2022/10/25/%EF%BC%88%E6%91%86%E7%83%82%E7%89%88%EF%BC%89Largrange-Dual-Problem/</url>
      
        <content type="html"><![CDATA[<p>由于椰奶棒冰实在是太懒了，有关拉格朗日对偶的问题就直接放纸质笔记了，打公式贼耗时间（</p><p>如果你识别图像中文字的算力不足，我的建议是赶快跳过这一章</p><p><img src="76A850D28E6629DB06E169E10717B2BE.png" alt="img"></p><p><img src="5A8F25F0DBB93BE99BB557BB5CC2595B.png" alt="img"></p><p><img src="6FB6307BAD09666A26D5BE0926794E2B.png" alt="img"></p><p><img src="BBCD8DBC739B43E2F9EE0A16F82B6226.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> notes </tag>
            
            <tag> Largrange Dual Problem </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计学习方法 6 逻辑斯蒂回归与最大熵模型</title>
      <link href="/2022/10/22/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-6-%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/"/>
      <url>/2022/10/22/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-6-%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h3 id="6-1-参数估计"><a href="#6-1-参数估计" class="headerlink" title="6.1 参数估计"></a>6.1 参数估计</h3><p>设：</p><p>$$<br>P(Y=1|x)=\pi(x),P(Y=0|x)= 1 - \pi(x)<br>$$</p><p>似然函数为：</p><p>$$<br>\Pi ^N _{i=1} [\pi(x_i)]^{y_i}[1- \pi(x_i)]^{1-y_i}<br>$$</p><p>对数似然函数为： </p><p>$$<br>L(w)=\sum ^N _{i=1} [y_i log \frac{\pi(x_i)}{1- \ pi(x_i)} + log(1 - \pi(x_i)) ]<br>$$</p><p>$$<br>=\sum ^N _{i=1} [y_i(w<em>x_i)-log(1+exp(w</em>x_i))]<br>$$</p><p>对 $w$ 求极大值，得到 $w$ 的估计值，可用梯度下降等</p><h3 id="6-1-4-多项逻辑斯蒂"><a href="#6-1-4-多项逻辑斯蒂" class="headerlink" title="6.1.4 多项逻辑斯蒂"></a>6.1.4 多项逻辑斯蒂</h3><p>即：softmax函数  </p><p>$$<br>P(Y=k|x)=\frac{exp(w_k*x)}{1+\sum ^K _{k=1}exp(w_k * x) }<br>$$<br>and<br>$$<br>P(Y=K|x)=\frac{1}{1+\sum ^{K-1} _{k=1}exp(w_k * x) }<br>$$</p><h3 id="6-2-最大熵模型"><a href="#6-2-最大熵模型" class="headerlink" title="6.2 最大熵模型"></a>6.2 最大熵模型</h3><p>maximum entropy model 认为，对一个随机事件的概率分布进行预测时，预测应当满足全部已知的约束，而对未知的情况不要做任何主观假设。在这种情况下，概率分布最均匀，预测的风险最小，因此得到的概率分布的熵是最大。<br>模型定义：  </p><ol><li>经验分布：通过训练数据T上进行统计得到的分布。我们需要考察两个经验分布,分别是x,y的联合经验分布以及x的分布<br>分别为：</li></ol><p>$$<br>\widetilde{P}(X=x,Y=y)= \frac{\nu (X=x,Y=y)}{N}<br>$$</p><p>$$<br>\widetilde{P}(X=x)=\frac{\nu (X=x)}{N}<br>$$</p><ol start="2"><li><p>特征函数$f(x,y)$：描述输入x和输出y间的某一事实<br>==1 则满足，==0则不满足</p></li><li><p>特征函数关于经验分布的期望</p></li></ol><p>$$<br>E_{\widetilde{P}}(f) = \sum _{x,y} \widetilde{P}(x,y)f(x,y)<br>$$</p><ol start="4"><li>特征函数关于模型和经验分布的期望值：<br>$$<br>E_P(f) = \sum _{x,y} \widetilde{P}(x)P(y|x)f(x,y)<br>$$</li></ol><p>定义在条件概率分布$P(Y|X)$上的条件熵为<br>$$<br>H(P)=-\sum _{x,y}\widetilde{P}(x)P(y|x)logP(y|x)<br>$$<br>最大的模型称为最大熵模型</p><h3 id="6-2-3-最大熵模型的学习"><a href="#6-2-3-最大熵模型的学习" class="headerlink" title="6.2.3 最大熵模型的学习"></a>6.2.3 最大熵模型的学习</h3><p>即求解最大熵模型的过程，可形式化为约束最优化问题：<br>$$<br>max_{P \in C} H(P)=-\sum _{x,y}\widetilde{P}(x)P(y|x)logP(y|x)<br>$$<br>s.t.</p><p>$$<br>E_P(f_i)-E_{\widetilde{P}}(f_i)=0 ,i=1,2,3…,n<br>$$<br>and<br>$$<br>\sum_y P(y|x)=1<br>$$<br>转化为无约束优化的对偶问题。</p>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> 统计学习方法 </tag>
            
            <tag> notes </tag>
            
            <tag> logistic </tag>
            
            <tag> 最大熵模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Implementation of Perceptron</title>
      <link href="/2022/10/18/Implementation-of-Perceptron/"/>
      <url>/2022/10/18/Implementation-of-Perceptron/</url>
      
        <content type="html"><![CDATA[<pre class="line-numbers language-none"><code class="language-none">import randomimport numpy as npimport matplotlib.pyplot as pltfrom scipy.special._precompute.gammainc_asy import etalearningrate = 0.5t = 100X = []Y = []W = 0B = 0#   二维的感知机算法def sample_point_gen():    #   先随机一条直线，用来确定点的标签Y    sample_line_w = random.random() * 20 - 10.00    sample_line_b = random.random() * 20 - 10.00    #   sample_line = sample_line_w * x + sample_line_b    #   随机生成一些点 X = [temp_x,temp_y] Y = 1 or -1 ,为标签    for iii in range(20):        temp_x = random.random() * 100 - 50        temp_y = random.random() * 100 - 50        X.append([temp_x,temp_y])        if sample_line_w * temp_x - temp_y + sample_line_b &gt;= 0:            Y.append(1)            plt.scatter(temp_x, temp_y, c='r')        else:            Y.append(-1)            plt.scatter(temp_x, temp_y, c='b')    return X, Yif __name__ == '__main__':    sample_point_gen()    eta = np.zeros((len(X[0]) + 1, 1))    error_list = []  # 误分点列表    # 开始训练    for _ in range(t):        error_count = 0        error_index = []        for i, x_i in enumerate(X):            y_i = eta[0] * x_i[0] + eta[1] * x_i[1] + eta[2]            # 如果该点被分类错误            if y_i * Y[i] &lt;= 0:                error_index.append(i)                error_count += 1        error_list.append(error_count)        if error_count &gt; 0:            i = random.choice(error_index)            eta[0] += learningrate * Y[i] * X[i][0]            eta[1] += learningrate * Y[i] * X[i][1]            eta[2] += learningrate * Y[i]    W = - eta[0] / eta[1]    B = - eta[2] / eta[1]    plot_y = W * X + B    plt.plot(X, plot_y, 'g', linewidth=1)    plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> tech </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> Perceptron </tag>
            
            <tag> 统计学习方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Insert $$ Smoothly</title>
      <link href="/2022/10/09/Insert-1228-Fluently/"/>
      <url>/2022/10/09/Insert-1228-Fluently/</url>
      
        <content type="html"><![CDATA[<p>We always type <code>$$</code> to inform the editor that we will edit formula in the next in put, but it’s quite complex cause we should type a <code>$</code> firstly then input some formulas and type another <code>$</code> finally.<br>However, in almost every editors, you can insert <code>()</code> readily, just press <code>(</code> and the editor will help you complete a <code>)</code> and move the cursor to the middle of them intimately.<br>So, how to config your VSCode to insert <code>$$</code> as smoothly as insert <code>()</code>?  </p><h2 id="Firstly"><a href="#Firstly" class="headerlink" title="Firstly"></a>Firstly</h2><p>Make sure you have download <code>LaTeX Workshop</code> in VSCode</p><h2 id="If-you-are-editing-tex"><a href="#If-you-are-editing-tex" class="headerlink" title="If you are editing *.tex"></a>If you are editing *.tex</h2><p>It’s simple, you just need to open <code>latex-language-configuration.json</code> and find <code>autoClosingPairs</code> then add <code>["$","$"]</code>  </p><h2 id="If-you-are-editing-md"><a href="#If-you-are-editing-md" class="headerlink" title="If you are editing *.md"></a>If you are editing *.md</h2><ul><li>press <code>ctrl+shift+p</code> and input <code>snippet</code> to open <code>Preferences: Configure User Snippets</code></li><li>choose <code>markdown.json</code></li><li>input <pre class="line-numbers language-none"><code class="language-none">"$$":{  "prefix": "$","body": "$$0$","description": "$$"}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> tech </category>
          
      </categories>
      
      
        <tags>
            
            <tag> markdown </tag>
            
            <tag> VSCode </tag>
            
            <tag> latex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI Introduction Chapter 4 Homework</title>
      <link href="/2022/10/08/AI-Introduction-Chapter-4-Homework/"/>
      <url>/2022/10/08/AI-Introduction-Chapter-4-Homework/</url>
      
        <content type="html"><![CDATA[<h3 id="4-1"><a href="#4-1" class="headerlink" title="4.1"></a>4.1</h3><p>$$<br>CF(E_2)=0.6 * 0.5=0.3<br>$$<br>$$<br>CF(E_4)=0.8* MIN (E_2 , E_3) = 0.8 * 0.3=0.24<br>$$<br>$$<br>CF_3(H)=0.24* 0.7=0.168<br>$$<br>$$<br>CF_4(H)=0.4*0.9=0.36<br>$$<br>由于 $CF_3(H)&gt;0,CF_4(H)&gt;0$ 故<br>$$<br>CF(H)=CF_3(H)+CF_4(H)-CF_3(H)*CF_4(H)=0.46752<br>$$</p><h3 id="4-4"><a href="#4-4" class="headerlink" title="4.4"></a>4.4</h3><p>$$<br>M(b)=M_1(b,c,d)<em>M_2(a,b)=0.7</em>0.6=0.42<br>$$<br>$$<br>M(b,c,d)=M_1(b,c,d)<em>M_2(a,b,c,d)=0.7</em>0.4=0.28<br>$$<br>$$<br>M(a,b)=M_1(a,b,c,d)<em>M_2(a,b)=0.3</em>0.6=0.18<br>$$<br>$$<br>M(a,b,c,d)=M_1(a,b,c,d)<em>M_2(a,b,c,d)=0.3</em>0.4=0.12<br>$$<br>其余的概率分配函数都为0</p><h3 id="4-5"><a href="#4-5" class="headerlink" title="4.5"></a>4.5</h3><p>$$<br>A\cap B=0.5/x_1+0.65/x_2+0.8/x_3+0.9/x_4+0.7/x_5<br>$$<br>$$<br>A\cup B=0.85/x_1+0.7/x_2+0.9/x_3+0.98/x_4+0.77/x_5<br>$$<br>$$<br>\neg A=0.15/x_1+0.3/x_2+0.1/x_3+0.9/x_4+0.7/x_5<br>$$</p><h3 id="4-7"><a href="#4-7" class="headerlink" title="4.7"></a>4.7</h3><p>0.4 0.8<br>0.4 0.9<br>0.7 0.5<br>0.7 0.6</p>]]></content>
      
      
      <categories>
          
          <category> solutions </category>
          
      </categories>
      
      
        <tags>
            
            <tag> solutions </tag>
            
            <tag> AI Introduction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>王说新语 0x01 椰奶棒冰锐评原神第一定律</title>
      <link href="/2022/10/01/%E7%8E%8B%E8%AF%B4%E6%96%B0%E8%AF%AD-0x01-%E6%A4%B0%E5%A5%B6%E6%A3%92%E5%86%B0%E9%94%90%E8%AF%84%E5%8E%9F%E7%A5%9E%E7%AC%AC%E4%B8%80%E5%AE%9A%E5%BE%8B/"/>
      <url>/2022/10/01/%E7%8E%8B%E8%AF%B4%E6%96%B0%E8%AF%AD-0x01-%E6%A4%B0%E5%A5%B6%E6%A3%92%E5%86%B0%E9%94%90%E8%AF%84%E5%8E%9F%E7%A5%9E%E7%AC%AC%E4%B8%80%E5%AE%9A%E5%BE%8B/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是原神第一定律"><a href="#什么是原神第一定律" class="headerlink" title="什么是原神第一定律"></a>什么是原神第一定律</h3><p><strong>原神第一定律</strong>，或称<strong>OP第一定律</strong>，在特定语境中可简称为<strong>第一定律</strong>，是指在<a href="www.bilibili.com">b站</a>的评论区中，若有逆天的评论出现，那么点开他的个人主页，往往会发现这是一个原神玩家。（当然，也不一定仅限b站，别的平台也是可以的）</p><h3 id="什么是OP"><a href="#什么是OP" class="headerlink" title="什么是OP"></a>什么是OP</h3><p><strong>OP</strong>，也叫<strong>原批</strong>，指的是<strong>逆天的</strong>原神玩家，名字的来源是著名的<a href="https://tieba.baidu.com/f?kw=%E6%8A%97%E5%8E%8B%E8%83%8C%E9%94%85">抗压背锅吧</a>在被OP爆破之后造出来的词。O，圆之意，取圆和原谐音的意思；P，批之意，在当代网络用语中，x批一般指x游戏的逆天玩家，一般含贬义，比如逆天的王者荣耀玩家就可称为<strong>农批</strong>，逆天的DOTA玩家就可称为<strong>刀批</strong>（当然，实际上一般称为刀斯林），显然的，逆天的原神玩家就可称为<strong>原批</strong>。当然在x批的定义中也不一定要逆天，有时候只要玩x游戏就可以满足该要求，有时也不含贬义，比如英雄联盟天才辅助<strong>beryl</strong>就是一名原神玩家，抗抗们就以OP来指代他。</p><h3 id="OP逆天言论行为大赏"><a href="#OP逆天言论行为大赏" class="headerlink" title="OP逆天言论行为大赏"></a>OP逆天言论行为大赏</h3><p>在分析OP逆天的原因之前，我们还是应该建立一些直观的感受，来欣赏一些OP逆天的言论</p><h4 id="感觉不如原神。。。画质"><a href="#感觉不如原神。。。画质" class="headerlink" title="感觉不如原神。。。画质"></a>感觉不如原神。。。画质</h4><p>我们所讨论的第一类叫做<strong>ky</strong>，<strong>ky</strong>，来源于日语<strong>空気が読めない</strong>发音的缩写，意思为说不合时宜的话，本文所讨论的ky，就是指OP在其他视频底下刷有关原神的评论，最著名的莫过于在原神的爸爸<strong>塞尔达传说</strong>下刷感觉<strong>不如原神。。。画质</strong>。</p><p>下图所展示的，是OP对于<strong>东方project</strong>（起源于1972年的古老ip）的碰瓷</p><p><img src="16315b389b504fc216d0e6daa0dde71191ef6d06.jpg" alt="没原神谁知道你们东方？"></p><p><img src="76dd37628535e5dd21a3f30033c6a7efcf1b6206.jpg" alt="定理求解"></p><p>又比如对于<strong>初音未来</strong>的碰瓷（一个诞生于2007年的世界著名ip）</p><p><img src="image-20221002002315297.png" alt="我去，初音未来！"></p><p><img src="image-20221002002430219.png" alt="定理求解"></p><p>以及前段时间偷了英雄联盟动画《双城之战》主题曲《孤勇者》</p><p><img src="image-20221002002808997.png" alt="感受抗抗们的火力吧！"></p><p><img src="image-20221002002849369.png" alt="定理求解"></p><h4 id="我去，你说的这个人是我同学（捂脸-png）"><a href="#我去，你说的这个人是我同学（捂脸-png）" class="headerlink" title="我去，你说的这个人是我同学（捂脸.png）"></a>我去，你说的这个人是我同学（捂脸.png）</h4><p>OP言论的第二个行为，则是缺乏知识及常识，无论是基本的生活常识和基本的社会常识</p><p>比如随意编造数值（被戳穿了还会急眼）</p><p><img src="331ce011b912c8fc39026014b9039245d488218b.jpg" alt="什么钢铁侠"></p><p>测绘错80cm</p><p><img src="006g43TSly1h6an3vrz0lj30o01hc40h.jpg" alt="你是在监狱里玩原神吗？"></p><p>比如碰瓷消防员和缉毒警察</p><p><img src="image-20221002003232370.png" alt="注：钟离是原神中的一个角色"></p><h4 id="原神的文化输出！"><a href="#原神的文化输出！" class="headerlink" title="原神的文化输出！"></a>原神的文化输出！</h4><p>OP的另一种逆天言论，就是对于米哈游的无限忠诚，对于原神的无限热爱。</p><p>比如觉得原神有优秀的文化输出。原神在国外的流水高这一点不假，事实上，只不过是一些穿着暴露的烧鸡罢了。</p><p><img src="006g43TSly1h6414i5d3sj30qw1e2q6z.jpg" alt="OP真的好急"></p><p>对于米哈游的无限忠诚，这当然体现在觉得米哈游是一家优秀的公司，米哈游很能赚钱至一点不假，至于产品的质量。。。呃呃</p><p>当然对于其他游戏公司（比如腾讯）的抹黑还是少不了的</p><p><img src="image-20221002010511737.png" alt="都是藤熏的阴谋!"></p><h4 id="偷个盘子先"><a href="#偷个盘子先" class="headerlink" title="偷个盘子先"></a>偷个盘子先</h4><p>以上都是OP的网上发言，当然少不了逆天的线下行为。</p><p>比如前段时间热度很高的原神与必胜客的联动，OP直接把画着原神角色的盘子偷了。</p><p><img src="006g43TSly1h5t5yqldb4j30wi0qd0uc.jpg" alt="简简单单偷个盘子"></p><p><img src="image-20221002010638382.png" alt="当然也不只是盘子"></p><h3 id="OP逆天的原因分析及原神这款游戏本身"><a href="#OP逆天的原因分析及原神这款游戏本身" class="headerlink" title="OP逆天的原因分析及原神这款游戏本身"></a>OP逆天的原因分析及原神这款游戏本身</h3><p>相信大家都已了解了OP是怎么样的一个群体，那么，他们为什么如此逆天呢？</p><p>我认为这和用户群体普遍的年龄偏小（极大量未走上社会的学生，包含大量的无知小学生）以及米哈游对于玩家的病态宣传有关，但这并不是本文的重点，在这里就不展开说了。</p><p>对于原神这款游戏本身，我的评价是值得称赞的地方有，比如是少有的多平台都可以玩的游戏（PC，移动端），变现能力也是有目共睹的，此外，对于3A大作外表的抄袭抄的还凑活，如果没有玩过3A大作的人对于这款游戏应该还是能有一点兴趣的。然而，仔细一看就会发现这是个一坨屎的游戏：莫名其妙的剧情，毫无新意的战斗方式技能，令人捉急的节奏,机械呆板的每日任务，甚至过场动画是没有跳过键的。如果一个人没有玩过正儿八经的3A而先遇到了原神（有一说一，由于用户的低龄性，这种人是真的多），那只能说他运气不太好碰到了这么一款弱智游戏，如果一个人玩过许多3A而觉得原神更好玩，那我觉得他的审美有点捉急，当然还有一种人是没空玩要花时间的好游戏而选择快餐式消费（压力巨大的今天，这种人也挺多），那我的评价是不如玩愤怒的小鸟。当然这个游戏的氪金机制也很弱智，角色靠肝基本时间很长，一氪就是一个648，花钱多的一批的同时没什么屌用。</p><h3 id="第一定律的数学证明"><a href="#第一定律的数学证明" class="headerlink" title="第一定律的数学证明"></a>第一定律的数学证明</h3><p>由上述分析我们已经建立了一个认知：玩原神的人，是个逆天的概率非常的大。但是这个认知显然是不太严谨的，我们当然希望能够通过数学工具，相对严谨地证明OP第一定律。</p><p>对于两种特征关联的可能性，那当然要从概率的角度解决问题，而概率问题有<strong>频率派</strong>和<strong>贝叶斯派</strong>之争，我认为在本问题中，频率派擅长的<strong>极大似然估计法</strong>并不太方便描述和证明该问题，那么我们就从贝叶斯派的处理<strong>最大后验概率</strong>的角度来半定量地证明OP第一定律。</p><h4 id="问题目标"><a href="#问题目标" class="headerlink" title="问题目标"></a>问题目标</h4><p>证明：当我们看到逆天言论时，他是原神玩家的概率很大。</p><h4 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h4><p>频率学派的观点是对总体分布做适当的假定，结合样本信息对参数进行统计推断，这里涉及总体信息和样本信息。而贝叶斯的核心思想，就是在此基础上引入了先验，即来源于人们常识和经验的信息。也就是说，和完全客观的频率派相比，贝叶斯派是有一丝主观的偏见在其中的。</p><p>于是我们便可得到贝叶斯公式<br>$$<br>后验=\frac{似然*先验}{证据}<br>$$<br>也就是<br>$$<br>P(A|B)=\frac{P(B|A)*P(A)}{P(B)}<br>$$<br>其中，$P(A|B)$代表在$B$发生的情况下$A$发生的概率，比如$P(我后选了鳄鱼|对面先选了亚索)$就代表对面选亚索的情况下我会选择玩鳄鱼的概率。</p><p>后验是我们要求的值，也是根据观察到的样本修正之后的概率值；似然相当于模型的参数，在贝叶斯的思想中模型也像所求的参数一样是不固定拥有变量的；证据则是前提$B$发生的概率。</p><h4 id="建立模型及对模型的分析"><a href="#建立模型及对模型的分析" class="headerlink" title="建立模型及对模型的分析"></a>建立模型及对模型的分析</h4><p>我们假定在接下来的讨论中，数据集是全体b站用户；同时，我们把玩原神的人称为<strong>原</strong>，把发表逆天言论的人称为<strong>逆</strong>，那么，根据贝叶斯公式我们可以得出：</p><p>$$<br>P(原|逆)=\frac{P(逆|原)*P(原) } {P(逆) }<br>$$</p><p>这么看其实并不是非常make sense，因为右边的多数参数并不能很好的比较大小。</p><p>这时我们注意到，全体b站用户这个群体可以简单分为玩原神的和不玩原神的，也就是</p><p>$$<br>b站全体用户=原+非原<br>$$<br>这样，我们使用全概率公式对上述式子进行改写，可以得到：</p><p>$$<br>P(原|逆)=\frac{ P(逆|原)*P(原) } { P(原)*P(逆|原)+P(非原)*P(逆|非原) }<br>$$<br>这样看来就直观很多了，我们首先看分子：</p><p>$P(逆|原)$的值是很大的，因为通过前文的描述，我们相信由于种种的原因（年龄小，米忽悠的宣传等等），原神玩家中逆天的比例是相当高的；$P(原)$的值也是很大的，因为在b站中，原神玩家本身的比例相对还是高的（玩家群体本身确实多，b站作为所谓的二次元圣地有大量的小鬼用户，b站作为原神的服务器之一带来了很多原本不玩b站的用户）。</p><p>我们再来看分母：</p><p>$P(非原)$是较小的，因为我们已经证明了 $P(原)$ 是比较大的，这么比较大小虽然不严谨，但问题不大，就算是五五开甚至是 $P(非原)$ 更大一些也不会特别影响结果；而 $P(逆|非原)$ ，也就是不玩原神的人中逆天的概率，笔者承认这种逆天的人也确实不少，但只凭原神大多数玩家都是undergraduate的以及因为许多人玩这个游戏变得paranoid，我们相信：$P(逆|非原)$的比例和 $P(逆|原)$ 相比还是比较小的。</p><p>$Q.E.D$.</p><h3 id="第一定律的应用以及与对证明的反思"><a href="#第一定律的应用以及与对证明的反思" class="headerlink" title="第一定律的应用以及与对证明的反思"></a>第一定律的应用以及与对证明的反思</h3><p>由于第一定律相当高的准确率，它被广泛运用于各种鉴定逆天的场景中，这一点我们暂且不提。</p><p>值得一提的是：由于第一定律的使用者有时缺乏足够的证据或者自身知识不足，会出现鉴定的人并不是逆天反而自己是逆天的情况，这往往出现在鉴定OP是否缺乏常识的时候。此时，我们将这种人称为：<strong>你和OP的最大区别就是你不玩原神</strong>。</p><p>此外，不玩原神的人中的逆天也是很多的，因此，第一定律并不是一个非常严格的定律，所以拿来图一乐就差不多得了。</p><p>最后笔者还想聊聊为什么反OP这个话题：显而易见的，是因为OP中的逆天非常的多，但这真的是原因吗？我们反对的当然是OP低下的素质以及魔怔般的行为。此外于个人情感而言，可以算，因为笔者作为一个undergraduate，身边的$P(逆|原)$还是比较大的，而且对于事物的看法并不是一定有一个唯一答案的，即使自己知道这样中庸的道理：原神玩家中有逆天也有不逆天，非原神玩家中有逆天也有不逆天。也许这应该是目前普世的一个看法或者说最接近真相的一个看法，然而，人之所以为人而不是机器人，因此有非标准答案又怎么样呢。正如王小波所说的：</p><blockquote><p>有些人认为，人应该充满境界高尚的思想，去掉格调低下的思想。这种说法听上去美妙，却使我感到莫大的恐慌。因为高尚的思想和低下的思想的总和就是我自己；倘若去掉一部分，我是谁就成了问题。假设有某君思想高尚，我是十分敬佩的；可是如果你因此想把我的脑子挖出来扔掉，换上他的，我绝不肯，除非你能够证明我罪大恶极，死有余辜。人既然活着，就有权保证他思想的连续性，到死方休。更何况那些高尚和低下完全是以他们自己的立场来度量的，假如我全盘接受，无异于请那些善良的思想母鸡到我脑子里下蛋，而我总不肯相信，自己的脖子上方，原来是长了一座鸡窝。想当年，我在军代表眼里，也是很低下的人，他们要把自己的思想方法、生活方式强加给我，也是一种脑移植。菲尔丁曾说，既善良又伟大的人很少，甚至是绝无仅有的，所以这种脑移植带给我的不光是善良，还有愚蠢。在此我要很不情愿地用一句功利的说法：在现实世界上，蠢人办不成什么事情。我自己当然希望变得更善良，但这种善良应该是我变得更聪明造成的，而不是相反。更何况赫拉克利特早就说过，善与恶为一，正如上坡和下坡是同一条路。不知道何为恶，焉知何为善？所以他们要求的，不过是人云亦云罢了。</p></blockquote><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
      
      
      <categories>
          
          <category> Wang&#39;s New Talk </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 原神 </tag>
            
            <tag> Bayes theorem </tag>
            
            <tag> probability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计学习方法 5 决策树</title>
      <link href="/2022/09/30/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-5-%E5%86%B3%E7%AD%96%E6%A0%91/"/>
      <url>/2022/09/30/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-5-%E5%86%B3%E7%AD%96%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<h2 id="5-1-概述"><a href="#5-1-概述" class="headerlink" title="5.1 概述"></a>5.1 概述</h2><p>包含三个步骤：特征选择，决策树的生成，决策树的修剪<br>决策树本质是从训练数据集中归纳出一组分类规则，需要找的是一个与训练数据矛盾小的决策树<br>用损失函数表示这一目标，通常是正则化的极大似然函数，决策树的学习策略是损失函数最小化<br>损失函数确定后，学习问题就变为在损失函数意义下选择最优决策树的问题<br>通常是递归地选择一个最优特征，并根据特征对训练数据分割<br>决策树生成考虑局部最优<br>决策树剪枝考虑全局最优  </p><h2 id="5-2-特征选择"><a href="#5-2-特征选择" class="headerlink" title="5.2 特征选择"></a>5.2 特征选择</h2><p>指决定用哪个特征来划分特征空间<br>而选择哪个，应该选择有最好分类的，用information gain来描述 </p><h3 id="5-2-1-information-gain-and-entropy"><a href="#5-2-1-information-gain-and-entropy" class="headerlink" title="5.2.1 information gain and entropy"></a>5.2.1 information gain and entropy</h3><p>设$X$是表示随机变量不确定性的度量，概率分布为<br>$$<br>P(X=x_i)=p_i,i=1,2,\dots,n<br>$$<br>则$X$的entropy定义为<br>$$<br>H(X)=-\sum^n_{i=1}{p_ilogp_i}<br>$$<br>由此可见，entropy只依赖于X的分布，与取值无关<br>entropy越大，随机变量的不确定性越大<br>且<br>$$<br>0\leq H(p) \leq logn<br>$$<br>同样类比可得conditional entropy的定义<br>$$<br>H(Y|X)=\sum _{i=1}^n{p_iH(Y|X=x_i)}<br>$$<br>当entropy由数据估计，如极大似然估计得到时，称为emprical entropy和empirical conditional entropy<br>information gain描述了得知$X$的信息而使得$Y$的信息不确定性减少的程度  </p><h3 id="5-2-2-def-information-gain"><a href="#5-2-2-def-information-gain" class="headerlink" title="5.2.2 def: information gain"></a>5.2.2 def: information gain</h3><p>特征$A$对训练数据集$D$的信息增益$g(D,A)$为<br>$$<br>g(D,A)=H(D)-H(D|A)<br>$$<br>称为mutual information  </p><h3 id="5-2-3-algorithm-information-gain"><a href="#5-2-3-algorithm-information-gain" class="headerlink" title="5.2.3 algorithm: information gain"></a>5.2.3 algorithm: information gain</h3><ul><li>计算$D$的emprical entropy$H(D)$</li><li>计算特征$A$对数据集$D$的empirical conditional entropy $H(D|A)$.<br>$$<br>H(D|A)=\sum ^n_{i=1}\frac{|D_i|}{|D|}H(D_i)=-\sum ^n_{i=1}\frac{|D_i|}{|D|}\sum ^K_{k=1}\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|}<br>$$</li><li>计算information gain </li><li>例题，见《统计学习方法》P75</li></ul><h3 id="5-2-4-information-gain-ratio"><a href="#5-2-4-information-gain-ratio" class="headerlink" title="5.2.4 information gain ratio"></a>5.2.4 information gain ratio</h3><p>由于上述方法倾向于选择特征多的，可用information gain ratio进行校准<br>即：<br>$$<br>g_R(D,A)=\frac{g(D,A)}{H_A(D)}<br>$$<br>其中，$H_A(D)=-\sum ^n_{i=1}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$,n<br>是特征$A$取值的个数</p><h2 id="5-3-决策树的生成"><a href="#5-3-决策树的生成" class="headerlink" title="5.3 决策树的生成"></a>5.3 决策树的生成</h2><h3 id="5-3-1-ID3"><a href="#5-3-1-ID3" class="headerlink" title="5.3.1 ID3"></a>5.3.1 ID3</h3><ul><li>input: 训练集$D$,特征集$A$,阈值$\epsilon$</li><li>output: 决策树$T$</li><li>IF $D$中所有实例属于同一类$C_k$，THEN $T$为单节点树，并将类$C_k$作为该节点的类标记，返回$T$</li><li>IF $A=\empty$,THEN $T$为单节点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$<ul><li>ELSE， 计算并选择信息增益最大的特征$A_g$</li></ul></li><li>IF $A_g$的信息增益 $&lt;$ 阈值$\epsilon$ THEN 置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$<ul><li>ELSE 对$A_g$的每一可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$</li></ul></li><li>对第$i$个子结点，以$D_i$为训练集，以$A-(A_g)$为特征集，递归地调用前$5$个步骤，得到子树$T_i$,返回$T_i$<br>该算法只有树的生成，故易过拟合</li></ul><h3 id="5-3-2-C4-5"><a href="#5-3-2-C4-5" class="headerlink" title="5.3.2 C4.5"></a>5.3.2 C4.5</h3><p>与$ID3$算法类似，把信息增益换成信息增益比即可  </p><h2 id="5-4-决策树的剪枝"><a href="#5-4-决策树的剪枝" class="headerlink" title="5.4 决策树的剪枝"></a>5.4 决策树的剪枝</h2><p>即简化生成树<br>往往通过极小化决策树整体的loss function或cost function实现，设树$T$的叶子节点为$|T|$个，$t$是$T$的叶结点，叶上有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$H_t(T)$为叶结点$t$上的经验熵，$\alpha &gt;0$为参数，则决策树学习的损失函数可以定义为：</p><p>$$<br>C_\alpha(T) = \sum _{t=1}^{|T|}N_tH_t(T)+\alpha |T|<br>$$</p><p>其中经验熵为  </p><p>$$<br>H_t(T) = -\sum <em>k \frac{N</em>{tk}}{N_t}log\frac{N_{tk}}{N_t}<br>$$</p><p>前者为模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$表示模型复杂度<br>剪枝，就是当$\alpha$确定时，选择损失函数最小的模型，即损失函数最小的子树<br>上述的损失函数极小化等价于正则化的极大似然估计<br>剪枝算法：</p><ul><li>计算每个结点的经验熵</li><li>递归地从树的叶结点往上回缩</li><li>如果某叶结点回缩到父节点后损失函数小了，就进行剪枝</li></ul><h2 id="5-5-CART-算法"><a href="#5-5-CART-算法" class="headerlink" title="5.5 CART 算法"></a>5.5 CART 算法</h2><p>CART = classification and regression tree</p><h3 id="5-5-1-CART-生成"><a href="#5-5-1-CART-生成" class="headerlink" title="5.5.1 CART 生成"></a>5.5.1 CART 生成</h3><p>即递归构建二叉决策树的过程，对回归树用平方误差最小化准则，对分类树用Gini index最小化准则进行特征选择，生成二叉树 </p><h4 id="回归树生成"><a href="#回归树生成" class="headerlink" title="回归树生成"></a>回归树生成</h4><p>一棵回归树对应输入空间的一个划分以及在划分的单元熵的输出值。假设已将输入空间划分为M个单元，并在每个单元$R_m$上有一个固定的输出值$c_m$,故回归树模型为<br>$$<br>f(x) = \sum ^m_{m=1}c_mI(x\in R_m)<br>$$<br>输入空间划分确定时，用平方误差$\sum _{x_i\in R_m}(y_i-f(x_i))^2$来表示回归树的预测误差，用平方误差最小求解每个单元上的最优输出值。  </p><h5 id="最小二乘回归树生成算法"><a href="#最小二乘回归树生成算法" class="headerlink" title="最小二乘回归树生成算法"></a>最小二乘回归树生成算法</h5><ul><li>选择最优切分变量$j$和切分点$s$,求解<br>$$<br>min_{j,s}[min_{c_1} \sum _{x_i\in R_1(j,s)}(y_i-c_1)^2 +min _{c_2} \sum _{x_i\in R_2(j,s)}(y_i-c_2)^2]<br>$$<br>遍历j  </li><li>用选定的对$(j,s)$划分区域并决定相应的输出值  </li><li>重复直到停止</li><li>将输入空间划分为M个区域生成决策树</li></ul><h4 id="分类树的生成"><a href="#分类树的生成" class="headerlink" title="分类树的生成"></a>分类树的生成</h4><p>用基尼指数选择最优特征，同时决定最优二值切分点<br>定义：<br>$$<br>Gini(p)=\sum ^K_{k=1}p_k(1-p_k)=1-\sum ^K_{k=1}p_k^2<br>$$<br>样本$D$根据特征$A$划分成$D_1$和$D_2$两部分，集合$D$的基尼系数定义为<br>$$<br>Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)<br>$$  </p><h3 id="CART-生成算法"><a href="#CART-生成算法" class="headerlink" title="CART 生成算法"></a>CART 生成算法</h3><ul><li>计算现有特征对该数据集的基尼指数</li><li>在所有可能的特征$A$及其所有可能的切分点$a$中选择基尼指数最小的特征及其对应切分点，依次生成子节点</li><li>递归</li><li>生成CART决策树<br>例：见P84</li></ul><h3 id="5-5-2-CART-剪枝"><a href="#5-5-2-CART-剪枝" class="headerlink" title="5.5.2 CART 剪枝"></a>5.5.2 CART 剪枝</h3><ul><li>input: CART算法生成的决策树$T_0$</li><li>output: 最优决策树$T_\alpha$</li><li>$k=0,T=T_0$</li><li>$\alpha = +\infty$</li><li>从下而上对内部结点$t$计算$C(T_t),|T_t|$以及<br>$$<br>g(t)=\frac{C(t)-C(T_t)}{|T_t|-1},\alpha = min(\alpha,g(t))<br>$$<br>这里，$T_t$是$t$为根结点的子树，$C(T_t)$是对训练数据的预测误差</li><li>对$g(t)=\alpha$ 的内部节点$t$进行剪枝，并对叶结点$t$以多数表决法决定其类，得到树$T$</li><li>$k=k+1,\alpha _k=\alpha,T_k=T$</li><li>如果$T_k$不是根结点及两个叶结点构成的树，回到<code>2</code>,否则$T_k=T_n$</li><li>交叉验证法在子树序列中选取最优的$T_\alpha$</li></ul>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> 统计学习方法 </tag>
            
            <tag> notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计学习方法 4 朴素贝叶斯法</title>
      <link href="/2022/09/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-4-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/"/>
      <url>/2022/09/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-4-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>naive Bayes 是基于贝叶斯定理和特征条件独立假设的分类方法<br>对于给定的训练数据集，先基于特征条件独立假设学习输入输出的联合概率分布，然后基于此模型，对给定的输入x，利用贝叶斯求出后验概率最大的输出y,即：通过样本数据学习特征基于类别的条件概率分布，然后求类别基于特征的概率<br>naive：指用于分类的特征在类确定的条件下是条件独立的，这样参数就对数级减少了。<br>即：<br>$$<br>P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},···,X^{(n)}=x^{(n)}|Y=c_k)<br>$$<br>$$<br>=\prod ^n_{j=1}(X^{(j)}=x^{(j)}|Y=c_k)<br>$$<br>将后验概率最大的类作为x的类输出，等价于<br>$$<br>y=argmax_{c_k}P(Y=c_k) \prod <em>j{P(X^{(j) } =x^{(j) } |Y=c_k) }<br>$$<br>求后验概率最大，等价于期望风险最小化，Loss function选择0-1函数<br>$$<br>f(x)=argmax</em>{y\in Y}P(y=c_k|X=x)<br>$$<br>这样就得到了后验概率最大化准则<br>$$<br>f(x)=argmax_{c_k}P(c_k|X=x)<br>$$</p><h3 id="naive-Bayes-algorithm-："><a href="#naive-Bayes-algorithm-：" class="headerlink" title="naive Bayes algorithm ："></a>naive Bayes algorithm ：</h3><p>先验概率的极大似然估计：<br>$$<br>P(Y=c_k)=\frac{\sum^N_{i=1}I(y_i=c_k)}{N},\quad k=1,2,\dots,K<br>$$<br>$$<br>P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum^N_{i=1}I(x_i^{(j)}=a_{jl},y_i=c_k)}{<br>sum^N_{i=1}I(y_i=c_k)}<br>$$<br>对于给定的实例x，计算<br>$$<br>P(Y=c_k)\prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_k)<br>$$<br>确定实例x的类<br>$$<br>y=argmax_{c_k}P(Y=c_k)\prod ^n_{j=1}{j=1}P(X^{(j)}=x^{(j)}|Y=c_k)<br>$$</p><h3 id="贝叶斯估计："><a href="#贝叶斯估计：" class="headerlink" title="贝叶斯估计："></a>贝叶斯估计：</h3><p>极大似然估计会产生概率为0，影响后验概率的计算结果，使分类产生偏差<br>采用贝叶斯估计：<br>即在分子上$+\lambda$,在分母上$+S_j\lambda<br>$</p>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> 统计学习方法 </tag>
            
            <tag> notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计学习方法 2 感知机</title>
      <link href="/2022/09/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-2-%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
      <url>/2022/09/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-2-%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="2-1-感知机模型"><a href="#2-1-感知机模型" class="headerlink" title="2.1 感知机模型"></a>2.1 感知机模型</h2><p>二类分类的线性分类模型，输出为$+1 -1$<br>定义：<br>$$<br>f(x)=sign(w^T · x + b)<br>$$<br>$w$为weight vector,b为bias  </p><h2 id="2-2-感知机学习策略"><a href="#2-2-感知机学习策略" class="headerlink" title="2.2 感知机学习策略"></a>2.2 感知机学习策略</h2><p>给定数据集的正实例点和负实例点能被超平面$S$:<br>$$<br>w^T · x + b = 0<br>$$<br>完全正确划分到超平面，则数据集是linearly separable data set<br>接下来讲学习策略:<br>先写出输入空间$R^n$任意一点$x_0$到超平面S的距离：<br>$$<br>\frac{1}{\Vert x \Vert_2}|w^T·x_0+b|<br>$$  </p><h3 id="2-2-1-几何意义的推导"><a href="#2-2-1-几何意义的推导" class="headerlink" title="2.2.1 几何意义的推导"></a>2.2.1 几何意义的推导</h3><p><img src="2022_09_27_09_17_18.png" alt="几何意义"><br>我们知道余弦夹角公式：<br>$$<br>cos\theta= \frac{w^Tx}{\Vert w \Vert \Vert x \Vert}<br>$$<br>从图中可以看出 $cos\theta \Vert x \Vert$ 是向量 $x$ 在$w$ 上的投影，即OA<br>也就是说，$w^Tx$ 是 $x$ 在 $w$ 上的投影 times w的长度 $\Vert w \Vert$.<br>$$<br>w^Tx=cos\theta \Vert x \Vert  \Vert w \Vert =-b<br>$$<br>$$<br>\Downarrow<br>$$<br>$$<br>cos\theta \Vert x\Vert = \frac{-b}{\Vert w \Vert}<br>$$<br>也就是说，投影长度OA=$\frac{-b}{\Vert w \Vert}$ 意味着：<br>$w^T · x + b=0$ 的几何意义：所有在w上投影长度为OA的向量集合，即图中的超平面<br>接下来推导平面上任意一点 $x’$ 到超平面S的距离d：<br>通过$d=BA=OA-OB$可知<br>$$<br>d=\frac{-b-w^Tx’}{\Vert w \Vert}<br>$$<br>最后，$b$ 可以理解为超平面的截距 </p><h3 id="2-2-2-损失函数"><a href="#2-2-2-损失函数" class="headerlink" title="2.2.2 损失函数"></a>2.2.2 损失函数</h3><p>因此，忽略1/$\Vert w \Vert$即可得到所有误分类点到超平面S的总距离,即损失函数的定义<br>$$<br>\sum_{x_i\in M}{y_i(w^Tx_i+b)}<br>$$ </p><h2 id="2-3-感知机学习算法"><a href="#2-3-感知机学习算法" class="headerlink" title="2.3 感知机学习算法"></a>2.3 感知机学习算法</h2><p>即求参数 $w,b$ ,使其为损失函数极小化问题的解<br>$$<br>min_{w,b}L(w,b)=-\sum_{x_i\in M}{y_i(w^Tx_i+b)} \tag{1}<br>$$<br>先选取一个超平面 $w_0,b_0$ 然后用梯度下降法极小化目标函数<code>(1)</code>（一次随机选取一个误分类点使其梯度下降）<br>损失函数的梯度由<br>$$<br>\nabla_w{L(w,b)}=-\sum_{x_i\in M}y_ix_i<br>$$<br>$$<br>\nabla_b{L(w,b)}=-\sum_{x_i\in M}y_i<br>$$</p><h2 id="2-4-感知机算法的对偶形式"><a href="#2-4-感知机算法的对偶形式" class="headerlink" title="2.4 感知机算法的对偶形式"></a>2.4 感知机算法的对偶形式</h2><p>基本想法是 将w和b表示成实例$x_i$和标记$y_i$的线性组合，通过求解系数得到w和b<br>对误分类点$(x_i,y_i)$ ($x_i$是数据坐标，$y_i$是标签)通过<br>$$<br>w \leftarrow w+\eta y_i x_i<br>$$<br>$$<br>b \leftarrow b+\eta y_i<br>$$<br>逐步修改w，b，假设修改n次，则w，b关于$(x_i,y_i)$的增量为$\alpha _i y_i x_i$ 和 $\alpha _iy_i$,其中$\alpha <em>i=n_i \eta$<br>$n_i$ 是第 $i$ 个样本点的更新次数<br>故<br>$$<br>w=\sum</em>{i=1}^N\alpha _i y_i x_i<br>$$<br>$$<br>b= \sum _{i=1}^N\alpha _i y_i<br>$$<br>$\eta = 1$时，表示第i个实例点由于误分而进行更新的次数，更新次数越多，意味着距离超平面越近，也越难正确分类</p><p>学习的目标从 $w,b$ 变成了 $n_i$<br>误分条件<br>$$<br>y_i(\sum^N_{j=1}\alpha_jy_jx_j·x_j+b)\leq 0<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> 统计学习方法 </tag>
            
            <tag> notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI Introduction Chapter 3 Homework</title>
      <link href="/2022/09/26/AI-Introduction-Chapter-3-Homework/"/>
      <url>/2022/09/26/AI-Introduction-Chapter-3-Homework/</url>
      
        <content type="html"><![CDATA[<h3 id="3-2"><a href="#3-2" class="headerlink" title="3.2"></a>3.2</h3><p>消去存在量词，化为skolem标准型后略去全称量词<br>$\neg P(x,f(x),z,g(x,z))$</p><h3 id="3-3"><a href="#3-3" class="headerlink" title="3.3"></a>3.3</h3><p>先去掉$\rightarrow$<br>$(\exists x)(\forall y)[\neg (\forall z)P(x,z) \vee R(x,y,f(a))]$<br>变量标准化后消去存在量词<br>$(\forall y)(\neg P(z,g(y)) \vee R(z,y,f(a)))$</p><h3 id="3-4"><a href="#3-4" class="headerlink" title="3.4"></a>3.4</h3><h4 id="3-4-1"><a href="#3-4-1" class="headerlink" title="3.4.1"></a>3.4.1</h4><p>直接消去全称量词和合取符号即可<br>$ {P(a,b),Q(c,d) } $</p><h4 id="3-4-3"><a href="#3-4-3" class="headerlink" title="3.4.3"></a>3.4.3</h4><p>先去掉$\rightarrow$<br>$(\forall x)(\exists y)(\neg (P(x,y) \vee Q(x,y) ) \vee R(x,y))$<br>移动否定符号到量词前面<br>$(\forall x)(\exists y)((\neg P(x,y) \wedge \neg Q(x,y) ) \vee R(x,y))$<br>利用分配律<br>$(\forall x)(\exists y)(\neg (P(x,y) \vee R(x,y) \wedge \neg Q(x,y) ) \vee R(x,y))$<br>去掉量词并使用skolem化<br>${ \neg (P(x,f(x)) \vee R(x,f(x)) \wedge \neg Q(y,f(y)) ) \vee R(y,f(y))  }$</p><h4 id="3-4-5"><a href="#3-4-5" class="headerlink" title="3.4.5"></a>3.4.5</h4><p>同第三问<br>${\neg P(x,y) \vee Q(x,y) \vee R(x,f(x,y)) }$</p><h4 id="3-4-7"><a href="#3-4-7" class="headerlink" title="3.4.7"></a>3.4.7</h4><p>同第三问<br>${\neg P(x,f(x)) \vee Q(x,g(x)),\neg P(y,f(y)) \vee \neg R(y,g(y)) }$</p><h3 id="3-7"><a href="#3-7" class="headerlink" title="3.7"></a>3.7</h3><p>定义谓词：A(x) 为 x 能够阅读；B(x) 为 x 有文化；C(x) 为 x 是海豚；D(x) 为 x 有智能<br>前提：$( \forall x)(A(x)\rightarrow B(x));(\forall y)(C(y) \rightarrow \neg B(y));(\exists z)(C(x) \wedge D(x))$<br>结论：$(\exists w)(D(w) \wedge \neg A(w))$<br>证明：<br>1 $\neg A(x)\vee B(x)$<br>2 $\neg C(y) \vee \neg B(y)$<br>3 $C(z)$<br>4 $D(z)$<br>5 $\neg D(w) \vee A(w)$<br>6 $A(m) \quad 4和5归纳$<br>7 $B(m) \quad 1和6归纳$<br>8 $\neg C(m) \quad 2和7归纳$<br>9 $(\exists w)(D(w) \wedge \neg A(w))\quad 3和8归纳得到结论$</p>]]></content>
      
      
      <categories>
          
          <category> solutions </category>
          
      </categories>
      
      
        <tags>
            
            <tag> solutions </tag>
            
            <tag> AI Introduction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>王说新语 0x00 椰奶棒冰锐评越战金曲Orange Crush</title>
      <link href="/2022/09/26/%E7%8E%8B%E8%AF%B4%E6%96%B0%E8%AF%AD-0x00-%E6%A4%B0%E5%A5%B6%E6%A3%92%E5%86%B0%E9%94%90%E8%AF%84%E8%B6%8A%E6%88%98%E9%87%91%E6%9B%B2Orange-Crus/"/>
      <url>/2022/09/26/%E7%8E%8B%E8%AF%B4%E6%96%B0%E8%AF%AD-0x00-%E6%A4%B0%E5%A5%B6%E6%A3%92%E5%86%B0%E9%94%90%E8%AF%84%E8%B6%8A%E6%88%98%E9%87%91%E6%9B%B2Orange-Crus/</url>
      
        <content type="html"><![CDATA[<h3 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h3><p>本文是对R.E.M乐队著名的反战歌曲Orange Crush的分析，如果你还没有听过，我建议你从<a href="https://www.youtube.com/watch?v=_mSmOcmk7uQ">这里</a>听一听，或者在<a href="https://www.youtube.com/watch?v=2BvXBwtrs_k">这里</a>欣赏一个优秀的live版本（可以看到主唱性感的光头），当然，边听边看本文也未尝不可。</p><p>前置知识：越战相关历史。</p><h3 id="完整歌词"><a href="#完整歌词" class="headerlink" title="完整歌词"></a>完整歌词</h3><blockquote><p>(Follow me, don’t follow me)<br>I’ve got my spine, I’ve got my orange crush<br>(Collar me, don’t collar me)<br>I’ve got my spine, I’ve got my orange crush<br>(We are agents of the free)<br>I’ve had my fun and now it’s time to serve your conscience overseas<br>(Over me, not over me)<br>Coming in fast, over me (oh, oh)</p></blockquote><blockquote><p>I’ve got my spine, I’ve got my orange crush<br>(Collar me, don’t collar me)<br>I’ve got my spine, I’ve got my orange crush<br>(We are agents of the free)<br>I’ve had my fun and now it’s time to serve your conscience overseas<br>(Over me, not over me)<br>Coming in fast, over me (oh, oh)</p></blockquote><blockquote><p>High on the roof, thin the blood<br>Another one came on the waves tonight<br>Comin’ in, you’re home</p></blockquote><blockquote><p>We would circle and we’d circle and we’d circle to stop and consider and centered on the pavement stacked up all the trucks jacked up and our wheels in slush and orange crush in pocket and all this here county, hell, any county, it’s just like heaven here, and I was remembering and I was just in a different county and all then this whirlybird that I headed for I had my goggles pulled off; I knew it all, I knew every back road and every truck stop</p></blockquote><blockquote><p>(Follow me, don’t follow me)<br>I’ve got my spine, I’ve got my orange crush<br>(Collar me, don’t collar me)<br>I’ve got my spine, I’ve got my orange crush<br>(We are agents of the free)<br>I’ve had my fun and now it’s time to serve your conscience overseas<br>(Over me, not over me)<br>Coming in fast, over me (oh, oh)</p></blockquote><blockquote><p>High on the roof, thin the blood<br>Another one climbs on the waves tonight<br>Comin’ in, you’re home</p></blockquote><blockquote><p>High on the roof, thin the blood<br>Another one climbs on the waves tonight<br>Comin’ in, you’re home</p></blockquote><p>注：（）内的是贝斯手的伴唱，中间长长一坨意义不明的话是主场拿扩音器喊话的部分</p><h3 id="对于歌曲的分析"><a href="#对于歌曲的分析" class="headerlink" title="对于歌曲的分析"></a>对于歌曲的分析</h3><blockquote><p>That didn’t mean you had to necessarily figure it out, you could just listen for pure enjoyment. But if you wanted to dig a little deeper there was always something there. That’s always been the case with my favorite songs.</p></blockquote><p>对于音乐性的部分没有什么好分析的，不是因为不值得分析，而是我觉得亲自听一听感受一下就足够了，单纯的技巧在rock and roll有重要性，但绝对不是最重要的部分。</p><p>如果一定要写一些，那我引用从 <strong>Classic Rock</strong>第185期里抄来的话</p><blockquote><p>The jangling rush of Peter Buck’s artful guitar, the wordless moan of their Mills-led harmonies, drummer Bill Berry’s deft rhythms and the delicious pull of Stipe’s cryptic vocals. <em>Orange Crush</em> had all these things, plus a killer punch and added sparkle. </p></blockquote><p>以及提一嘴歌中使用的模仿颚骨的打击乐器</p><blockquote><p>This features the rattle of a vibraslap in the mix. The percussion instrument was created to mimic the sound of a jawbone, an instrument made from the dried jawbone of a donkey, horse, or similar animal, whose teeth would produce a rattling sound when shaken. The modern version uses a stiff wire to connect a wooden ball to a box of metal teeth.</p></blockquote><h3 id="对于歌词的分析"><a href="#对于歌词的分析" class="headerlink" title="对于歌词的分析"></a>对于歌词的分析</h3><blockquote><p>The ironic juxtaposition of those two terms was no accident.</p></blockquote><p>前文已经提过了，这是一首反战歌曲，标题中的Orange Crush指代的是Agent Orange，即<strong>橙剂</strong>，一种在<strong>越战</strong>中被美军滥用的落叶剂（用于找出隐藏于树林中越共），美军对自己的士兵称橙剂对人是无害的，然而几年后却发现实质上是一种致癌物。对于Orange Crush这个标题本身的分析我并没有找到什么靠谱的说法，个人认为可以理解为Orange Crush的另一个意思：一种当时流行的橙味汽水 与 橙剂 形成强烈的讽刺对比</p><p><img src="637ef30b1424183947f70ed7ee7dfafc.355x355x1.jpg" alt="crush 橙汁"></p><p>接下来我们看对于每一句歌词的考证（没写的代表我没找到或者没想出来）<br>本歌曲的口吻主要是一名越战美军士兵的第一视角</p><pre class="line-numbers language-none"><code class="language-none">(Follow me, don't follow me)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在越南冲突中，经常有一名士兵被派往部队前方检查危险或伏击的情况。<br><code>Follow me</code>是他的部队在他的位置上前进的暗示<br>而<code>Don't follow me</code>是停止运动和观察的以下指示。当然这句话随时间也有了一些别的意思：越战进行期间，彻底失败的美国士兵们讽刺“Don’t Follow Me… I’m Lost too.”意为对战争的不满。</p><pre class="line-numbers language-none"><code class="language-none">I've got my spine<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>对于这一句的<code>spine</code>有两种理解，一种是士兵说他有他的<code>spine</code>，即他不受橙剂的影响，同时相信他的孩子也会没事（当然，实际上橙剂导致多种癌症，并且会遗传给后代，比如导致子女的<strong>脊柱裂</strong>等疾病；另一种理解是指LSD和amphetamine的混合（就是吸毒）。</p><pre class="line-numbers language-none"><code class="language-none">I've got my orange crush<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>关于橙剂已经在前文分析过了，至于<code>我获得了橙剂</code>，应该是士兵做好了发动空袭的准备（因为橙剂的使用方式是空投）。</p><pre class="line-numbers language-none"><code class="language-none">(Collar me, don't collar me)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><code>collar</code>指的是用来固定脊柱裂患者的支架和项圈，这里指的是脊柱裂发生的不确定性，以及士兵不确定负面影响的真相。</p><p><img src="58b032bf3ca331b7201d964e8eb57cbb.257x320x1.jpg" alt="collar"></p><pre class="line-numbers language-none"><code class="language-none">(We are agents of the free)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这里指的是美国士兵相信自己是自由的代言人，相信美国在越南的战争是为了当地带来自由的，</p><pre class="line-numbers language-none"><code class="language-none">I've had my fun and now it's time to serve your conscience overseas<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>同上一句。越南战争的开始，是因为美国政客的<code>conscience </code>说，美国需要干预共产主义在亚洲的蔓延。这一句应该是对美国政客说的，至于<code>I've had my fun</code>，可能是对于参加越战的一种反讽。</p><pre class="line-numbers language-none"><code class="language-none">(Over me, not over me)Coming in fast, over me<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>伴唱对主唱<code>now it's time to serve your conscience overseas</code>的回答<code>在我身上，而不是在海外</code>，所以伴唱有点与既定的爱国主义观点相矛盾。</p><pre class="line-numbers language-none"><code class="language-none">High on the roof, thin the bloodAnother one came on the waves tonightComin'in, you're home<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>很明显是指越南很糟糕，士兵们对战争理由的怀疑，恐惧，士兵们只想回家，祈祷一架直升机来拯救他们离开这个地狱。</p><pre class="line-numbers language-none"><code class="language-none">We would circle and we'd circle and we'd circle to stop and consider and centered on the pavement stacked up all the trucks jacked up and our wheels in slush and orange crush in pocket and all this here county, hell, any county, it's just like heaven here, and I was remembering and I was just in a different county and allthen this whirlybird that I headed for I had my goggles pulled off; I knew it all, I knew every back road and every truck stop<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这是中间主唱拿扩音器喊的一坨意义不明的话，同时周围还环绕着直升机的声音，内容主要是对于战场的描述，<br>在主题上，类似于《现代启示录》中“我喜欢早上凝固汽油弹的味道”独白。插曲和独白都描述了说话者对暴力战时环境的愉悦，特别强调他们使用致命化学武器对付敌人的乐趣。</p><pre class="line-numbers language-none"><code class="language-none">Another one came on the waves tonightComin' in, you're home<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>在越南冲突期间，从战区撤离伤亡人员（CASEVAC）是通过带有呼号“Dustoff”的直升机进行的。<br>当然<code>Another one came on the waves tonight</code>也有另外一层含义，指的是将一名士兵的尸体运回美国。</p><h3 id="写在后面"><a href="#写在后面" class="headerlink" title="写在后面"></a>写在后面</h3><p>至于为什么有些能分析出士兵对战争的厌恶而有些能表现出战争的决心，而且这两者在歌曲中是交错出现的，我还没能想出一个合理的解释，总不至于说是以此来表达内心的矛盾吧（因为歌曲总体是反战的）。</p>]]></content>
      
      
      <categories>
          
          <category> Wang&#39;s New Talk </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Vietnam War </tag>
            
            <tag> rock and roll </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计学习方法 1 概论</title>
      <link href="/2022/09/25/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-1-%E6%A6%82%E8%AE%BA/"/>
      <url>/2022/09/25/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-1-%E6%A6%82%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="1-基本分类"><a href="#1-基本分类" class="headerlink" title="1 基本分类"></a>1 基本分类</h2><h3 id="1-1-监督学习"><a href="#1-1-监督学习" class="headerlink" title="1.1 监督学习"></a>1.1 监督学习</h3><ul><li>监督学习从training data中学习模型，对test data进行预测，训练数据由输入输出对组成. </li><li>联合概率分布：假设输入输出的随机变量$X$和$Y$遵循联合概率分布$P(X,Y)$.</li><li>监督学习的模型是概率模型或非概率模型.由条件概率分布$P(Y|X)$或decision function $Y=f(x)$表示.</li><li>其实就是<strong>分类问题</strong></li></ul><h3 id="1-2-无监督学习"><a href="#1-2-无监督学习" class="headerlink" title="1.2 无监督学习"></a>1.2 无监督学习</h3><p>从无标注数据中学习预测模型  </p><ul><li>假设$X$是输入空间，$Z$是隐式结构空间，要学习的模型表示为$z=g(x)$或$P(z|x)$的形式.</li><li>每一个样本是一个实例.</li><li>就是学习之后获得一个模型，which try to find 数据内在的联系，典型的例子是聚类分析.</li></ul><h3 id="1-3-强化学习"><a href="#1-3-强化学习" class="headerlink" title="1.3 强化学习"></a>1.3 强化学习</h3><ul><li><p>指智能系统在和环境的连续互动中学习最优策略的问题  </p></li><li><p>本质是学习最优的序贯策略  </p></li><li><p>强化学习的马尔可夫决策过程是状态，奖励，动作序列上的随机过程$[S,A,P,r,\gamma]$，下一个状态$P(s’|s,a)$只依赖于前一个状态和动作,下一个奖励$r(s,a)$只依赖于前一个状态和动作</p></li><li><p>价值函数 value function 是策略 $\pi$ 从某一个状态 $s$ 开始的长期积累奖励的数学期望</p></li><li><p>动作价值函数 action value function 定义和价值函数差不多</p></li><li><p>强化学习的目标是在所有策略中选择出价值函数最大的策略 $\pi^*$.</p></li></ul><h2 id="2-按模型分类"><a href="#2-按模型分类" class="headerlink" title="2 按模型分类"></a>2 按模型分类</h2><ul><li>概率与非概率：这个前面讲过了，其中概率模型一定可以用最基本的加法规则和乘法规则进行概率推理.</li><li>线性与非线性：取决于使用的函数性质</li></ul><h2 id="3-按算法分类"><a href="#3-按算法分类" class="headerlink" title="3 按算法分类"></a>3 按算法分类</h2><ul><li>在线学习：一次输入一个样本后进行一次预测，比如利用随机梯度下降的感知机算法</li><li>批量学习：一次学完并预测模型</li></ul><h2 id="4-按技巧分类"><a href="#4-按技巧分类" class="headerlink" title="4 按技巧分类"></a>4 按技巧分类</h2><ol><li>贝叶斯学习</li></ol><ul><li>特点是使用模型的先验分布计算后验概率<br>  $$<br>P(\theta|D)=\frac{P(\theta)P(D|\theta)}{P(D)}<br>  $$</li><li>估计时，估计整个后验概率$P(\theta | D)$, 给出的模型往往是后验概率最大的模型</li><li>预测时，计算数据对后验概率分布的期望值<br>$$<br>P(x|D)=\int P(x|\theta,D)P(\theta | D)d\theta<br>$$</li><li>和频率派的联系：假设先验分布是均匀分布，取后验概率最大，就能从贝叶斯估计得到极大似然估计</li></ul><ol start="2"><li>核方法</li></ol><ul><li>定义一个函数，使得能够把线性模型扩展到非线性模型，在高维特征空间中进行内积计算，从而简化计算.</li></ul><h2 id="5-三要素"><a href="#5-三要素" class="headerlink" title="5 三要素"></a>5 三要素</h2><p>$$<br>方法 = 模型 + 策略 + 算法<br>$$</p><ul><li>模型：决策函数表示的叫非概率模型，条件概率表示的叫概率模型</li><li>策略：<ul><li><p>损失函数和风险函数 0-1，平方，绝对，对数等损失函数，值越小模型越好，即学习的目标</p></li><li><p>经验风险最小化:即求解最优化问题: (典型的有极大似然估计等)(容易过拟合)<br>$$<br>min_{f\in F}\frac{1}{N}\sum^N_{i=1}L(y_i,f(x_i))<br>$$</p></li><li><p>结构风险最小化：(等价于正则化，即加上了表示模型复杂度的正则化项$J(F)$)(比如贝叶斯估计中的最大后验概率估计)<br>$$<br>R_{srm}(f)=\frac{1}{N}\sum^N_{i=1}L(y_i,f(x_i)) + \lambda J(f)<br>$$</p></li></ul></li></ul><h2 id="6-训练误差与测试误差"><a href="#6-训练误差与测试误差" class="headerlink" title="6 训练误差与测试误差"></a>6 训练误差与测试误差</h2><ul><li>训练误差：训练数据集的平均损失</li><li>测试误差：测试数据集的平均损失</li></ul><h2 id="7-正则化与交叉验证"><a href="#7-正则化与交叉验证" class="headerlink" title="7 正则化与交叉验证"></a>7 正则化与交叉验证</h2><p>见 <code>5</code> 中关于正则化项的描述<br>回归问题中，正则化项可以是参数向量的$L_2$或$L_1$范数<br>符合Occam’s razor原理：在所有可能模型中，能很好解释已知数据并且十分简单的才是最好的模型<br>从贝叶斯角度：正则化项对应于模型的先验概率，可以假设复杂的模型有小的先验，简单的模型有大的先验</p><ul><li>S折交叉验证：随机将数据分成S个互不相交大小相同的子集，利用S-1个子集的数据训练模型，剩下的子集测试，重复这一过程，最后选出S次测评中平均测试误差最小的模型。</li></ul><h2 id="8-泛化能力"><a href="#8-泛化能力" class="headerlink" title="8 泛化能力"></a>8 泛化能力</h2><p>泛化能力：对未知数据的预测能力<br>往往研究泛化误差的概率上界generalization error bound,即比较两种学习方法的泛化误差上界的大小<br>性质：样本容量增加，泛化上界趋于0，假设空间容量越大，模型越难学，泛化误差上界就越大  </p><h2 id="9-生成模型和判别模型"><a href="#9-生成模型和判别模型" class="headerlink" title="9 生成模型和判别模型"></a>9 生成模型和判别模型</h2><p>我的理解：<br>生成模型：求出产生数据的条件概率分布$P(Y|X)$<br>判别模型：求出对数据进行分类的函数</p>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> 统计学习方法 </tag>
            
            <tag> notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Insert Images to Your Blog</title>
      <link href="/2022/09/22/insert-images-to-your-blog/"/>
      <url>/2022/09/22/insert-images-to-your-blog/</url>
      
        <content type="html"><![CDATA[<h2 id="Notice"><a href="#Notice" class="headerlink" title="Notice:"></a>Notice:</h2><p>This article is only apply to those who store images locally.</p><h2 id="Principle-of-inserting-images-in-hexo"><a href="#Principle-of-inserting-images-in-hexo" class="headerlink" title="Principle of inserting images in hexo"></a>Principle of inserting images in hexo</h2><p>Firstly, you should turn on <code>post_asset_folder</code> and <code>Hexo</code> will generate a folder with the same name as each article, then <code>Hexo</code> will link the images in the folder to the corresponding article. For example, you create an article called <strong>lbwnb.md</strong> and <code>Hexo</code> will create a folder called <strong>lbwnb</strong> automatically. Then you put <strong>17killandeatchicken.jpg</strong> in the folder and write <code>!(p1)[17killsandeatchicken.jpg]</code> in <strong>lbwnb.md</strong>, <code>Hexo</code> will just show the image in your blog instead of this line of code.<br>However, it’s inconvenient for us to do these steps, you should put the images in the folder, copy the name of the image and write the code like <code>!(p1)[17killsandeatchicken.jpg]</code>. Here I will show you 2 ways to insert images.</p><h2 id="Insert-images-via-Typora-recommend"><a href="#Insert-images-via-Typora-recommend" class="headerlink" title="Insert images via Typora (recommend)"></a>Insert images via Typora (recommend)</h2><p>It’s quite simple in Typora to do that.<br>You just need to open the image setting and set <code>copy image to custom folder</code> to <code>/$(filename)$</code>, then when you paste images to your articles, it will automatically store it in the  corresponding folder.</p><p><img src="image-20220922160233295.png" alt="image 1"></p><h3 id="Insert-images-via-VSCode-not-recommend"><a href="#Insert-images-via-VSCode-not-recommend" class="headerlink" title="Insert images via VSCode (not recommend)"></a>Insert images via VSCode (not recommend)</h3><p>However, it’s difficult to insert images in VSCode.</p><p>First, you should open the whole folder in VSCode.</p><p><img src="image-20220922160851370.png" alt="image 2"><br>Then, because of VSCode plugins don’t support set variables in storage path (default path is /images ), so you have to move the images in <code>/images</code> to the corresponding folder. </p><p><img src="image-20220922160549159.png" alt="image 3"></p><p>Finally, delete <code>/images</code> in the reference path, such as replacing <code>![iamge 13](images/2022_09_22_04_17_08.png)  </code> with <code>![iamge 13](2022_09_22_04_17_08.png)</code> </p><p>​</p>]]></content>
      
      
      <categories>
          
          <category> tech </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VSCode </tag>
            
            <tag> images </tag>
            
            <tag> Typora </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CSAPP Chapter2 Solution</title>
      <link href="/2022/09/20/CSAPP-Chapter2-Solution/"/>
      <url>/2022/09/20/CSAPP-Chapter2-Solution/</url>
      
        <content type="html"><![CDATA[<h3 id="2-7"><a href="#2-7" class="headerlink" title="2.7"></a>2.7</h3><p>it only print <code>61 62 63 64 65 66</code> cause <code>strlen</code> doesn’t print the last character <code>\</code>  </p><h3 id="2-10"><a href="#2-10" class="headerlink" title="2.10"></a>2.10</h3><table><thead><tr><th>step</th><th>*x</th><th>*y</th></tr></thead><tbody><tr><td>start</td><td>a</td><td>b</td></tr><tr><td>step1</td><td>a</td><td>a^b</td></tr><tr><td>step2</td><td>b</td><td>a^b</td></tr><tr><td>step3</td><td>b</td><td>a</td></tr></tbody></table><h3 id="2-11"><a href="#2-11" class="headerlink" title="2.11"></a>2.11</h3><p>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;A. <code>first = last = k</code>, it’s obvious.<br>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;B. That’s because the swap function we call is based on bitwise XOR operation, however, we get <code>0</code> when we calculate <code>a^a</code>, so it’s unsuitable to use that function.<br>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;C. Just replace <code>first&lt;=last</code> with <code>first&lt;last</code> .</p><h3 id="2-12"><a href="#2-12" class="headerlink" title="2.12"></a>2.12</h3><p>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;A. <code>x &amp; 0xFF</code><br>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;B. <code>~x ^ 0xFF</code> or <code>x ^ ~0xFF</code><br>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;C. <code>x | 0xFF</code>  </p><h3 id="2-13"><a href="#2-13" class="headerlink" title="2.13"></a>2.13</h3><ul><li><code>bis(x,y)</code>  </li><li><code>bis(bic(x,y),bic(y,x))</code>  </li><li>It’s obvious that <code>bix(x,y)</code> is equal to OR operation and <code>bic(x,y)</code> is equal to <code>x + ~y</code>  .</li></ul><h3 id="2-1-9-notes"><a href="#2-1-9-notes" class="headerlink" title="2.1.9 notes:"></a>2.1.9 notes:</h3><ul><li>shift logical right:   fill <code>k</code> zeros on the left  </li><li>arithmetic shift right: fill <code>k</code> highest order number on the left</li></ul><h3 id="2-15"><a href="#2-15" class="headerlink" title="2.15"></a>2.15</h3><p><code>!(x^y)</code>  </p><h3 id="2-25"><a href="#2-25" class="headerlink" title="2.25"></a>2.25</h3><p><code>length</code> is <code>unsigned int</code> so <code>length - 1 == MAXINT</code><br>how to modity:  </p><ul><li>replace <code>unsighed int</code> with <code>int</code><br>or  </li><li>replace <code>i &lt;= length - 1</code> with <code>i &lt; length</code></li></ul><h3 id="2-26"><a href="#2-26" class="headerlink" title="2.26"></a>2.26</h3><ul><li>when: when <code>strlen(s) &lt; strlen(t)</code></li><li>why: because <code>strlen</code> is <code>unsighed</code> and it will return a very big positive number if  <code>strlen(s) &lt; strlen(t)</code>  </li><li>how: change the return code to <code>return strlen(s) &gt; strlen(t)</code></li></ul><h3 id="2-27"><a href="#2-27" class="headerlink" title="2.27"></a>2.27</h3><pre class="line-numbers language-none"><code class="language-none">int uadd_ok(unsigned x, unsigned y){  unsighed sum = x+y;  return sum &gt;= x;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-30"><a href="#2-30" class="headerlink" title="2.30"></a>2.30</h3><pre class="line-numbers language-none"><code class="language-none">int tadd_ok(int x,int y){  if((x&gt;0 &amp;&amp; y&gt;0 &amp;&amp; x+y&lt;=0)+(x&lt;0 &amp;&amp; y&lt;0 &amp;&amp; x+y&gt;=0)==1)return 0;  return 1;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-31"><a href="#2-31" class="headerlink" title="2.31"></a>2.31</h3><p>Whether <code>x + y</code> overflows or not doesn’t affect the result because <code>complementary code addition</code> forms an <code>abelian group</code> so the answer is fixed 0. </p>]]></content>
      
      
      <categories>
          
          <category> solutions </category>
          
      </categories>
      
      
        <tags>
            
            <tag> solutions </tag>
            
            <tag> CSAPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PRML Probability Distributions</title>
      <link href="/2022/09/19/ML-Probability-Distributions/"/>
      <url>/2022/09/19/ML-Probability-Distributions/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Bernoulli-Distribution"><a href="#1-Bernoulli-Distribution" class="headerlink" title="1. Bernoulli Distribution"></a>1. Bernoulli Distribution</h2><h3 id="1-1"><a href="#1-1" class="headerlink" title="1.1"></a>1.1</h3><p>If we are tossing coins.<br>Let<br>$$<br>p(x=1|\mu )=\mu \\<br>p(x=0|\mu )=1-\mu \tag{1}<br>$$<br>Where 0 $\leq$ $\mu$ $\leq$ 1<br>So it’s easy to know<br>$$<br>E[x]= \mu \\<br>var[x]=\mu (1-\mu) \tag{2}<br>$$<br>Let<br>$D$ = {$x_1$ ,…,$x_N$} as  observed values of x, we can get the likelihood function and differentiate it<br>$$<br>ln(p(D|\mu))= \sum_{n=1}^Nln(p(x_n|\mu))= \sum_{n=1}^N{x_nln\mu+(1-x_n)ln(1-\mu)}\tag{3}<br>$$<br>set the derivative of $ln(p(D|\mu))=0$, we obtain the maximum likelihood estimator.<br>$$<br>\mu_{ML}=\frac{1}{N}\sum_{n=1}^Nx_n  \tag{4}<br>$$<br>In order to obtain thenormalization coefficient we note that out of N coin flips, we have to add up all of the possible ways of obtaining m heads, so that the binomial distribution can be written<br>$$<br>Bin(m|N,\mu)={N\choose m}\mu ^m(1-\mu)^{N-m}\tag{5}<br>$$</p><h3 id="1-2-Mean-and-Variance"><a href="#1-2-Mean-and-Variance" class="headerlink" title="1.2 Mean and Variance"></a>1.2 Mean and Variance</h3><p>Thus, we have<br>$$<br>E[m]=\sum_{m=0}^N{mBin(m|N,\mu)}=N\mu \tag{6}<br>$$<br>$$<br>var[m]=\sum_{m=0}^N{(m-E[m])^2}Bin(m|N,\mu)=N\mu(1-\mu)  \tag{7}<br>$$  </p><h2 id="2-Beta-Distribution"><a href="#2-Beta-Distribution" class="headerlink" title="2. Beta Distribution"></a>2. Beta Distribution</h2><h3 id="2-1"><a href="#2-1" class="headerlink" title="2.1"></a>2.1</h3><p>In Binary Variables, we can see if the dataset is too small, it may lead to serious overfitting, so we should consider a form of prior distribution, which made me consider beta distribution as a probability’s probability or the density of the success rate of Bernoulli trials.<br>Let a = number of successes, b = number of failures (we are still tossing coins) and we can choose a prior, called the beta distribution, given by  </p><p>$$<br>Beta( \mu | a , b ) = \frac{ \Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}=\frac{1}{B(a,b) } \mu ^ { a-1} (1- \mu )^{b-1}  \tag{8}<br>$$</p><p>B(a,b), also called Beta function, ensures that the beta distribution is normalized, so that<br>$$<br>    \int_0^1Beta(\mu|a,b)d\mu=1 \tag{9}<br>$$</p><h3 id="2-2-Mean-and-Variance"><a href="#2-2-Mean-and-Variance" class="headerlink" title="2.2 Mean and Variance"></a>2.2 Mean and Variance</h3><p>Then we can get the mean and the variance</p><p>$$<br>    E[\mu]=\int_0^1Beta(x|a,b)dx=\int_0^1x\frac{x^{a-1}(1-x)^{b-1}}{Beta(a,b)}dx\\<br>    =\frac{B(a+1,b)}{B(a,b)}\\<br>    =\frac{\Gamma(a+1)\Gamma(b)\Gamma(a+b)}{\Gamma(a+b+1)\Gamma(a)\Gamma(b)}\\<br>    =\frac{a}{a+b}<br>\tag{10}<br>$$</p><p>To calc $Var[\mu]$, we should calc $E[\mu^2]$ first beacuse $Var[\mu]=E[\mu^2]-E[\mu]^2$.</p><p>$$<br>E[\mu^2]=\int_{-\infty}^{+\infty}{x^2}{Beta(x)dx}\\<br>=\int_0^1{x^2}\frac{1}{B(a,b)}{x^{a-1}}{(1-x)^{b-1}}dx\\<br>=\frac{B(a+2,b)}{B(a,b)}\<br>=\frac{\Gamma(a+2)\Gamma(b)}{\Gamma(a+b+2)}\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\\<br>=\frac{(a+1)a}{(a+b+1)(a+b)}\tag{11}<br>$$  </p><p>Then we can get $Var[\mu]$ readily<br>$$<br>Var[\mu]=\frac{(a+1)a}{(a+b+1)(a+b)}-(\frac{a}{a+b})^2\<br>=\frac{ab}{(a+b+1)(a+b)^2} \tag{12}<br>$$</p><p>If our target is to better predict next result of $x$, that’s equal to pretict<br>$$<br>p(x=1|D)=\int_0^1p(x=1|\mu)p(\mu|D)d\mu\<br>=\int_0^1\mu p(\mu|D)d\mu\<br>=E[\mu|D] \tag{13}<br>$$<br>So we can get<br>$$<br>p(x=1|D)=\frac{m+a}{m+a+l+b} \tag{14}<br>$$<br>It tell us when the number of tests ( m , l ) is big enough, the result is <strong>Maximum Likelihood Estimate</strong>.<br>For limited dataset, the estimate of $\mu$ is between prior and  Maximum Likelihood Estimate.</p><h2 id="3-The-Gaussian-Distribution"><a href="#3-The-Gaussian-Distribution" class="headerlink" title="3. The Gaussian Distribution"></a>3. The Gaussian Distribution</h2><h3 id="3-1-The-Gaussian-Distribution"><a href="#3-1-The-Gaussian-Distribution" class="headerlink" title="3.1 The Gaussian Distribution"></a>3.1 The Gaussian Distribution</h3><p>As we all know, the Gaussian distribution can be written in the form of<br>$$<br>N( x | \mu , \sigma^2 )=\frac{1} {(2\pi\sigma^2)^ {1/2} }exp{ {-\frac{1}{2\sigma^2}(x-\mu)^2} } \tag{15}<br>$$<br>where $\mu$ is the mean and $\sigma^2$ is the variane.<br>If x is $D$-dimensional vector x, it take the form<br>$$<br>N ( x | \mu , \Sigma ) = \frac{1}{(2 \pi)^ {D/2} } \frac {1} { |\Sigma| ^ {1/2} }exp{ {-\frac{1}{2}(x-\mu)^T\Sigma^{-1} (x-\mu) } }   \tag  {16}<br>$$<br>Let<br>$$<br>\Delta^2=(x-\mu)^T\Sigma^{-1}(x-\mu) \tag{17}<br>$$<br>and the quantity $\Delta$ is called the <strong>Mahalanobis distance</strong><br>from µ to x and reduces to the Euclidean distance when Σ is the identity matrix.   </p><p>Usually, we assume $\Sigma$ is a <strong>symmetric matrix</strong>, so we can <strong>Singular Value Decomposition</strong> it.<br><strong>Geo expression of Gaussian Distribution</strong>:<br>Imagine an elliptical surface of constant probability density for a Gaussian in a two-dimensional space x = ($x_1,x_2$) on which the density is exp(−1/2) of its value at x = $\mu$. The major axes of the ellipse are defined by the eigenvectors $u_i$ of the covariance matrix, with corresponding eigenvalues $\lambda_i$ .</p><h3 id="3-2-Conditional-Gaussian-Distribution"><a href="#3-2-Conditional-Gaussian-Distribution" class="headerlink" title="3.2 Conditional Gaussian Distribution"></a>3.2 Conditional Gaussian Distribution</h3><p>An important property of the multivariate Gaussian distribution is that if two<br>sets of variables are jointly Gaussian, then the conditional distribution of one set<br>conditioned on the other is again Gaussian.<br>Split $x$ in two  </p><p>$$<br>x={x_a \choose x_b}<br>$$</p><p>$$<br>\mu= {\mu_{a }\choose\mu_{b } }<br>$$</p><p>$$<br>\Sigma = {\Sigma_{aa } \quad \Sigma_{ab } \choose \Sigma_{ba } \quad \Sigma_{bb } }<br>$$<br>Where $\mu$ is mean and $\Sigma$ is covariance matrix.<br>Let<br>$$<br>\Lambda = \Sigma ^{-1 }<br>$$<br>which is known as the <strong>precision matrix</strong><br>so<br>$$<br>\Lambda = {\Lambda_{aa } \quad \Lambda_{ab } \choose \Lambda_{ba } \quad \Lambda_{bb } }<br>$$</p><p>We just need remember the mean and variance of conditional Gaussian distribution<br>$$<br>\mu_{a|b } = \mu _a + \Sigma _{ab } + \Sigma _{bb }^{-1} (x_b-\mu _b )   \tag{18}<br>$$</p><p>$$<br>\Sigma_{a|b } = \Sigma_{aa}- \Sigma_{ab } \Sigma_{bb } ^{-1} \Sigma_{ba }  \tag{19}<br>$$<br>Prove:<br>We don’t use the method in PRML but another constructive method raised by <a href="https://www.bilibili.com/video/BV1aE411o7qd">shuhuai008</a><br>Let<br>$$<br>x_{b·a}=x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a \tag{20}<br>$$<br>$$<br>\mu_{b·a}=\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a \tag{21}<br>$$<br>$$<br>\Sigma_{bb·a } = \Sigma_{bb } - \Sigma_{ba } \Sigma_{aa } ^ {-1 } \Sigma_{ab } \tag{22}<br>$$<br>We call $\Sigma_{bb·a }$ is $\Sigma_{aa }$’s <strong>Schur Complementary</strong><br>Rewrite <code>(20)</code><br>$$<br>x_{b·a } = (-\Sigma_{ba }\Sigma_{aa }^{-1 } \quad I_m){x_a \choose x_b} \tag{23}<br>$$<br>As we all know, if $x$<del>$N(\mu,\Sigma)$, $y=Ax+b$, then $y$</del>$N(A\mu +B,A\Sigma A^{-1 })$<br>So $-\Sigma_{ba}\Sigma_{aa}^{-1 }$ is $A$ we are finding.<br>Then we can calc<br>$$<br>E[x_{b·a }] = (-\Sigma_{ba }\Sigma_{aa }^{-1 } \quad I_m){\mu_a \choose \mu_b } = \mu_{b·a} \tag{24}<br>$$<br>$$<br>Var[x_{b·a } ]=(-\Sigma_{ba }\Sigma_{aa }^{-1 } \quad I_m){\Sigma_{aa } \quad \Sigma_{ab } \choose \Sigma_{ba } \quad \Sigma_{bb } } { {-\Sigma_{ba } \Sigma_{aa}^{-1 } } \choose {I_m } } = \Sigma_{bb·a} \tag{25}<br>$$</p><h3 id="3-3-Marginal-Gaussian-Distribution"><a href="#3-3-Marginal-Gaussian-Distribution" class="headerlink" title="3.3 Marginal Gaussian Distribution"></a>3.3 Marginal Gaussian Distribution</h3><p>We have seen that if a joint distribution $p(x_a,x_b)$ is Gaussian,then the conditional distribution $p(x_a|x_b)$ will again be Gaussian. Now we turn to a discussion of the marginal distribution given by</p><p>$$<br> p(x_a) = \int p(x_a,x_b)dx_b<br>$$<br>we can get<br>$$<br>E[x_a] = \mu_a \tag{26}<br>$$</p><p>$$<br>cov[x_a]=\Sigma_{aa} \tag{27}<br>$$</p><p>prove:<br>target:  value of  $p(y)$ $p(x|y)$<br>for $p(y)$<br>let<br>$$<br>y=Ax+b+\epsilon<br>$$<br>and $\epsilon$<del>$N(0,L^{-1 } )$,so<br>$$<br>E[y]=E[Ax+b+\epsilon ]=E[Ax+b]+E[\epsilon]=A\mu+b<br>$$<br>$$<br>Var[y]=Var[Ax+b+\epsilon]=Var[Ax+b]+Var[\epsilon]<br>$$<br>$$<br>=A·\Lambda^{-1 } ·A^{T } +L^{-1 } \tag{28}<br>$$<br>so $y$</del>$N(A\mu+b,A·\Lambda^{-1 } ·A^{T } + L^{-1 } )$<br>for $p(x|y)$<br>let<br>$$<br>z={x \choose y } \sim N({\mu \choose A\mu +b},{\Lambda^{-1 } \quad \Lambda^{-1 }A^{T }  \choose A^{T }\Lambda^{-1 } \quad A·\Lambda^{-1 } ·A^{T } + L^{-1 }} )<br>$$<br>so the result is<br>$$<br>E[x|y] = \mu + \Lambda^{-1 }A^{T }(A·\Lambda^{-1 } ·A^{T } + L^{-1 } )^{-1 } (y-A\mu-b)<br>$$<br>$$<br>Var[x|y]=\Lambda^{-1 } - \Lambda^{-1 }A^{T }(A·\Lambda^{-1 } ·A^{T } + L^{-1 } )^{-1 } A\Lambda^{-1 } \tag{29}<br>$$</p><h3 id="3-4-Student’s-t-distribution"><a href="#3-4-Student’s-t-distribution" class="headerlink" title="3.4 Student’s t-distribution"></a>3.4 Student’s t-distribution</h3><p>It equal to combined by many Gaussian distribution with same mean and diffenert variance.</p><h2 id="4-The-Exponential-Family"><a href="#4-The-Exponential-Family" class="headerlink" title="4 The Exponential Family"></a>4 The Exponential Family</h2><p>The probability distributions that we have studied so far in this chapter (with the<br>exception of the Gaussian mixture) are specific examples of a broad class of distri-<br>butions called <strong>the exponential family</strong><br>The exponential family of distributions over x, given parameters η, is defined to<br>be the set of distributions of <strong>normal form</strong><br>$$<br>p(x|\eta) = h(x)g(\eta)exp(\eta^Tu(x)) \tag{30}<br>$$<br>$u(x)$ is called <strong>sufficient statistic</strong>, which store all of the message of the whole dataset.<br>$\eta$ is called <strong>natural parameter</strong> of the distribution, when $p(x|\theta)$ is limited,  the set of $\eta$ is called <strong>natural parameter space</strong>.<br>$g(\eta)$ is called <strong>log-partition function</strong>, which function is to normalize the int.<br>Where<br>$$<br>g( \eta ) \int h(x) exp[ \eta^T u(x) ]dx = 1<br>$$<br>The target of ExpFamily is rearrangement different distributions into <strong>normal forms</strong>.  </p><h3 id="4-1-Bernoulli-distribution"><a href="#4-1-Bernoulli-distribution" class="headerlink" title="4.1  Bernoulli distribution"></a>4.1  Bernoulli distribution</h3><p>$$<br>p(x|\mu) = Bern(x|\mu) = \mu^x(1-\mu)^{1-x }<br>$$<br>$$<br>=(1-\mu)exp[ln(\frac{\mu } {1-\mu })x]<br>$$<br>let<br>$$<br>\eta = ln(\frac{\mu }{1-\mu } )<br>$$<br>rearrangement it and let $\mu = \sigma(\eta)$<br>$$<br>\sigma(\eta)=\frac{1 } {1+exp(-\eta) }<br>$$<br>which is called <strong>logistic sigmoid function</strong><br>Thus we can write the Bernoulli distribution using the standard representation in the form<br>$$<br>p(x|\eta)=\sigma(-\eta)exp(\eta x)<br>$$</p><h3 id="4-2-Gaussian-distribution"><a href="#4-2-Gaussian-distribution" class="headerlink" title="4.2  Gaussian distribution"></a>4.2  Gaussian distribution</h3><p>$$<br>p( x | \mu , \sigma^2 )=\frac{1} {(2\pi\sigma^2)^ {1/2} }exp{ {-\frac{1}{2\sigma^2}(x-\mu)^2} }<br>$$<br>$$<br>=\frac{1}{(2\pi \sigma^2)^{1/2}}exp(-\frac{1}{2\sigma^2}(-2\mu \quad 1){x \choose x^2} -\frac{\mu^2}{2\sigma^2})<br>$$<br>So comparing to <code>30</code><br>$$<br>\eta = {\frac{\mu }{ \sigma^2 } \choose -\frac{1 } {2\sigma^2 } } = {\eta_1 \choose \eta_2 }<br>$$<br>$$<br>u(x)={x \choose x^2}<br>$$<br>$$<br>h(x)=(2\pi)^{-1/2}<br>$$<br>$$<br>g(\eta)=(-2\eta_2)^{1/2}exp(\frac{\eta_1^2}{4\eta_2})<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> notes </tag>
            
            <tag> PRML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI Introduction Chapter1,2 Homework</title>
      <link href="/2022/09/18/AI-Introduction-Chapter1-2-Homework/"/>
      <url>/2022/09/18/AI-Introduction-Chapter1-2-Homework/</url>
      
        <content type="html"><![CDATA[<h3 id="1-1"><a href="#1-1" class="headerlink" title="1.1"></a>1.1</h3><p>人工智能就是用人工的方法在计算机上实现的智能，也成为机器智能.<br>发展阶段：</p><ul><li>孕育：1956年之前  </li><li>形成：1956-1969年</li><li>发展：1970年后</li></ul><h3 id="1-2"><a href="#1-2" class="headerlink" title="1.2"></a>1.2</h3><ul><li>可形式化  </li><li>存在算法  </li><li>有限时间内可求解</li></ul><h3 id="1-3"><a href="#1-3" class="headerlink" title="1.3"></a>1.3</h3><ul><li>知识表示: 包括符号表示法和连接机制表示法</li><li>机器感知: 以机器视觉和机器听觉为主  </li><li>机器思维: 指对通过感知得来的外部信息及机器内部的各种工作信息进行有目的的处理  </li><li>机器学习: 研究如何使计算机具有类似人类的学习能力，在学习中自动获取知识  </li><li>机器行为: 指计算机的表达能力</li></ul><h3 id="1-4"><a href="#1-4" class="headerlink" title="1.4"></a>1.4</h3><ul><li>自动定理证明  </li><li>博弈  </li><li>模式识别  </li><li>机器视觉  </li><li>自然语言理解  </li><li>智能信息检索  </li><li>数据挖掘与知识发现  </li><li>专家系统  </li><li>自动程序设计  </li><li>机器人  </li><li>组合优化问题  </li><li>人工神经网络  </li><li>分布式人工智能与多智能体  </li><li>智能控制  </li><li>智能仿真  </li><li>智能CAD  </li><li>智能CAI  </li><li>智能管理与智能决策  </li><li>智能多媒体系统  </li><li>智能操作系统  </li><li>智能通信  </li><li>智能网络系统  </li><li>人工生命</li></ul><h3 id="1-5"><a href="#1-5" class="headerlink" title="1.5"></a>1.5</h3><p>“擅智”与“善智”，指利用人工智能增强国家治理能力和促使人工智能成为“良善的技术”  </p><hr><h3 id="2-1"><a href="#2-1" class="headerlink" title="2.1"></a>2.1</h3><p>在本题中，我们统一定义: x: 人</p><ol><li>定义谓词: like(x,y) 为x喜欢y.  f1: 梅花 f2: 菊花.<br>$\exists$ x (likes(x,f1)) $\vee$ ($\exists$ x)(likes(x,f2)) $\vee$ $\exists$ x(likes(x,f1) $\wedge$ likes(x,f2))  </li><li>定义谓词: plays(l,m,n) 为 l在m的时候玩n<br>$\forall$ 下午(plays(他,足球,下午))  </li><li>定义谓词: have(x,y) 为x有y eat(x,z) 为x吃z<br> $\forall$ x(have(x,饭) $\wedge$ eat(x,饭))  </li><li>定义谓词: likes(x,y) 为x喜欢y play(x,z) 为x打z<br> $\forall$ x(like(x,(play(x,篮球))) $\rightarrow$ like(x,(play(x,排球))))  </li><li>定义谓词: exam(x,l) 为x参加l科目的考试 pass(x,m) 为x通过m科目的考试 study(x,n) 为x到n地点学习<br>$\forall$ x(study(x,国外) $\rightarrow$ pass(x,exam(x,英语)))</li></ol><h3 id="2-2"><a href="#2-2" class="headerlink" title="2.2"></a>2.2</h3><ol><li>$\forall$ x的辖域是P(x,y) $\vee$ $\exists$ y(Q(x,y) $\wedge$ R(x,y))<br> $\exists$ y 的辖域是(Q(x,y) $\wedge$ R(x,y))<br>谓词公式中x是约束变元<br>P(x,y)中y是自由变元<br>$\exists$ y (Q(x,y) $\wedge$ R(x,y)中的y是约束变元  </li><li>$\exists$ z和 $\forall$ y 的辖域都是P(z,y) $\vee$ Q(z,x)<br>谓词公式中y,z是约束变元，x,u,v是自由变元  </li><li>$\forall$ x的辖域是$\neg$ P(x,f(x)) $\vee$ $\exists$ z(Q(x,z) $\wedge$ $\neg$ R(z,y))<br> $\exists$ z的辖域是Q(x,z) $\wedge$ $\neg$ R(z,y)<br>谓词公式中x,z是约束变元，y是自由变元  </li><li>$\forall$ z的辖域是 $\exists$ y( $\exists$ t(P(z,t) $\vee$ Q(y,t)) $\wedge$ R(z,y))<br>$\exists$ y的辖域是 $\exists$ t (P(z,t) $\vee$ Q(y,t))<br>$\exists$ t的辖域是P(z,t) $\vee$ Q(y,t)<br>   谓词公式中z和t为约束变元<br>   P(z,t) $\vee$ Q(y,t)中的y为约束变元<br>   R(z,y)中的y为自由变元</li></ol><h3 id="2-3"><a href="#2-3" class="headerlink" title="2.3"></a>2.3</h3><p>一个解释：<br>P(1,1)=T;P(1,2)=T;P(2,1)=T;P(2,2)=T;Q(1,1)=T;Q(1,2)=T;Q(2,1)=T;Q(2,2)=T;<br>{x,y}={1,1}时,P=T,Q=T,真值为T<br>{x,y}={2,1}时,P=T,Q=T,真值为T<br>{x,y}={1,2}时,P=T,Q=T,真值为T<br>{x,y}={2,2}时,P=T,Q=T,真值为T<br>故该谓词公式为永真式  </p><h3 id="2-4"><a href="#2-4" class="headerlink" title="2.4"></a>2.4</h3><ol><li>older(x,y): x的年龄比y大<br>older(张三,李四) $\rightarrow$ $\neg$ older(张三,李四)  </li><li>marry(x,y): x和y结婚 male(x): x为男 $\neg$ male(x): x为女<br>marry(x,y) $\rightarrow$ (man(甲) $\wedge$ $\neg$ man(乙)) $\vee$ (man(乙) $\wedge$ $\neg$ man(甲))  </li><li>honest(x): x是老实人 lie(x): x说谎了<br> honest(x) $\rightarrow$ $\neg$ lie(x);<br>lie(张三) $\rightarrow$ $\neg$ honest(张三);</li></ol><h3 id="2-5"><a href="#2-5" class="headerlink" title="2.5"></a>2.5</h3><p>IF $X_1$=0 AND $X_2$=0 THEN $Y$=0;<br>IF $X_1$=0 AND $X_2$=1 THEN $Y$=1;<br>IF $X_1$=1 AND $X_2$=0 THEN $Y$=1;<br>IF $X_1$=1 AND $X_2$=1 THEN $Y$=0;  </p><h3 id="2-6"><a href="#2-6" class="headerlink" title="2.6"></a>2.6</h3><ul><li>框架名：地震  <ul><li>地震属性<ul><li>地点：下斯洛文尼亚地区  </li><li>强度：里氏8.5级  </li><li>时间：今天</li></ul></li><li>后果  <ul><li>25人死亡和5亿美元的财产损失</li></ul></li><li>事后总结  <ul><li>发言人：下斯洛文尼亚地区的主席  </li><li>发言内容：多年来，靠近萨迪壕金斯断层的重灾区一直是一个危险地区。这是本地区发生的第3号地震。</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> solutions </category>
          
      </categories>
      
      
        <tags>
            
            <tag> solutions </tag>
            
            <tag> AI Introduction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CF1591C Solution</title>
      <link href="/2022/09/16/CF1591C-Solution/"/>
      <url>/2022/09/16/CF1591C-Solution/</url>
      
        <content type="html"><![CDATA[<p>&amp;emsp;&amp;emsp;At first I thought the method is dp or classification discussion, but they are all very copmlex.<br>&amp;emsp;&amp;emsp;So I found it can be solved in reverse. Notice that we can transport <strong>k</strong> goods each time and we don’t consider coming back first. <strong>The whole distance</strong> is equal to start from the endpoints on both sides respectively and each time the <strong>k</strong>th good as the end point of each sub-distance.<br>&amp;emsp;&amp;emsp;It’s easy to find out that if we don’t do like this, the endpoint must be sent once, then the point closest to the endpoint also must be sent once. In this condition, the whole distance is definitely longer then the last one. I don’t want to prove it because it’s trivial.<br>&amp;emsp;&amp;emsp;Then we consider how to back to the startpoint. We just need <strong>2</strong> <strong>*</strong> <strong>formeranswer</strong>  <strong>-</strong> <strong>the longset sub-distance</strong>.<br>&amp;emsp;&amp;emsp;As for how to know the longset sub-distance, we just need to maintenance an array when we were calculating the sub-distance, or just sort it.<br>Here is the whole code.  </p><pre class="line-numbers language-none"><code class="language-none">#include &lt;bits/stdc++.h&gt;using namespace std; typedef long long ll;typedef double db;const ll mod=1e9+7; ll a[200050];ll b[200050];int main(){std::ios::sync_with_stdio(false);cin.tie(0);cout.tie(0);//freopen("cfsb.out","w",stdout);int T;cin&gt;&gt;T;while(T--){ll n,k;cin&gt;&gt;n&gt;&gt;k;for(int i=1;i&lt;=n;i++)cin&gt;&gt;a[i];sort(a+1,a+1+n);ll num=0;for(ll i=1;i&lt;=n&amp;&amp;a[i]&lt;0;i+=k)b[++num]=-a[i];    for(ll i=n;i&gt;=1&amp;&amp;a[i]&gt;0;i-=k)b[++num]=a[i];sort(b+1,b+num+1);ll ans=0;for(int i=1;i&lt;=num;i++)ans+=2*b[i];ans-=b[num];cout&lt;&lt;ans&lt;&lt;endl;}    return 0;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> solutions </category>
          
      </categories>
      
      
        <tags>
            
            <tag> solutions </tag>
            
            <tag> codeforces </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to Solve Code Block Appearance Exception</title>
      <link href="/2022/09/16/How-to-Solve-Code-Block-Appearance-Exception/"/>
      <url>/2022/09/16/How-to-Solve-Code-Block-Appearance-Exception/</url>
      
        <content type="html"><![CDATA[<h2 id="Error"><a href="#Error" class="headerlink" title="Error"></a>Error</h2><p>When I insert code block to my articles, I found the code blocks could’t be rendered correctly.<br>They are quite <strong>ugly</strong> and all the functions, such as folding, copying and shrinking, don’t work.  </p><h2 id="Reason"><a href="#Reason" class="headerlink" title="Reason"></a>Reason</h2><p>The highlight function can’t work correctly.  </p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><ul><li><p>Open <strong>_config.yml</strong> and find <strong>highlight</strong> and <strong>prismjs</strong>, then config them like this:</p><pre class="line-numbers language-none"><code class="language-none">highlight:  enable: false  line_number: true  auto_detect: false  tab_replace: ''  wrap: true  hljs: falseprismjs:  enable: true  preprocess: true  line_number: true  tab_replace: ''<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>Uninstall <strong>hexo-prism-plugin</strong></p><pre class="line-numbers language-none"><code class="language-none">npm uninstall hexo-prism-plugin<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> tech </category>
          
      </categories>
      
      
        <tags>
            
            <tag> matery </tag>
            
            <tag> debug </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>About this Blog</title>
      <link href="/2022/09/15/How-this-blog-is-built/"/>
      <url>/2022/09/15/How-this-blog-is-built/</url>
      
        <content type="html"><![CDATA[<p>Inspired by <a href="https://baokker.github.io/">baokker</a>, I decided to bulid my own blog three days ago. Here is a brief introduction, which partly references <a href="https://letian.website/githubpageshexodajianjingtaiboke/">this</a> and <a href="https://hexo.io/zh-cn/docs/configuration">this</a>.</p><h2 id="Register-a-Github-account"><a href="#Register-a-Github-account" class="headerlink" title="Register a Github account"></a>Register a Github account</h2><p>This step is quite easy, and then creat a repository named your <strong>own username + .github.io</strong>, this repository will be the place where you will store your blog’s code in the future. </p><h2 id="Download-Git-and-Node-js-and-configure-SSH-Key"><a href="#Download-Git-and-Node-js-and-configure-SSH-Key" class="headerlink" title="Download Git and Node.js and configure SSH-Key"></a>Download Git and Node.js and configure SSH-Key</h2><p><a href="https://www.cnblogs.com/mingyue5826/p/11141324.html">Tutorial</a></p><ol><li>Download Git from <a href="https://git-scm.com/downloads">here</a> and download Node.js from <a href="https://nodejs.org/en/">here</a></li><li>Check whether PATH is configured</li><li>If you can’t find “Git bash here” in the right-click menu, have a look at <a href="https://blog.51cto.com/u_15301254/3057144">this guide</a></li><li>Enter the following command to setup your <strong>username</strong> and <strong>email</strong><pre class="line-numbers language-none"><code class="language-none">git config --global user.name git config --global user.email <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li>Generate <strong>SSH-Key</strong> and name it as <strong>id_rsa_gitlab</strong><pre class="line-numbers language-none"><code class="language-none">cd ~/.sshssh-keygen -t rsa -C "yourmail@glanway.com"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li>Add private key  <pre class="line-numbers language-none"><code class="language-none">ssh-add ~/.ssh/id_rsa_githubssh-add ~/.ssh/id_rsa_gitlab<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li>If the Git error: <strong>Could not open a connection to your …</strong></li></ol><ul><li>enter <code>ps aux | grep ssh</code> to check the ssh-agent</li><li>the thread number is the first number of last command’s results, then enter <code>kill -9 "thread number"</code> to kill the thread  </li><li>put the private key into <strong>/.ssh</strong><pre class="line-numbers language-none"><code class="language-none">cd ~/.sshexec ssh-agent basheval ssh-agent -sssh-add ./id_rsa_githubssh-add ./id_rsa_gitlab<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><ol start="8"><li>Create and modify <strong>config.txt</strong><pre class="line-numbers language-none"><code class="language-none">Host github.comHostName github.comPreferredAuthentications publickeyIdentityFile ~/.ssh/id_rsa_githubUser yenaibangbing<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li>Generate the single ssh-key</li></ol><pre class="line-numbers language-none"><code class="language-none">ssh-keygen -t rsa -C "yourmail@xxxl.com"<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="10"><li>Add the public key to github<br>Find <strong>SSH and GPG keys</strong> in the settings and put <strong>*.pub</strong> into it, which begins with <strong>ssh-rsa</strong>  </li><li>Test<br>If you see <strong>Hi!…</strong> after enter <code>ssh -T git@github.com</code>, that means you’ve successfully set up the SSH-Key. ^-^</li></ol><h2 id="Download-hexo-and-create-an-empty-fold"><a href="#Download-hexo-and-create-an-empty-fold" class="headerlink" title="Download hexo and create an empty fold"></a>Download <strong>hexo</strong> and create an empty fold</h2><ul><li>You can download <strong>hexo</strong> from <a href="https://hexo.io/zh-cn/docs/">here</a> </li><li>Create an <strong>empty</strong> fold and name it as “blog”, then <code>hexo init</code>  </li><li>Modify <strong>_config.yml</strong> under the guide of <a href="https://hexo.io/zh-cn/docs/configuration">this</a></li></ul><h2 id="Choose-a-theme-you-like"><a href="#Choose-a-theme-you-like" class="headerlink" title="Choose a theme you like"></a>Choose a theme you like</h2><p>Different themes have different configs, so I suggest you google it or read the README.md of your chosen theme.<br>My theme is called <strong>hexo-theme-matery</strong>, you can get it form <a href="https://github.com/blinkfox/hexo-theme-matery">github</a> or just click the link under my blog.  </p><h2 id="Deploy-it-to-Github"><a href="#Deploy-it-to-Github" class="headerlink" title="Deploy it to Github"></a>Deploy it to Github</h2><p>Firstly, enter these two commands  </p><pre class="line-numbers language-none"><code class="language-none">hexo cleanhexo g<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>If you want to view it locally, just <code>hexo s</code> and open the link hexo returned.<br>Then enter <code>hexo d</code> to deploy the whole fold to Github.<br>Now, everyone is able to visit your blog.</p><h2 id="CDN-accerlerating"><a href="#CDN-accerlerating" class="headerlink" title="CDN accerlerating"></a>CDN accerlerating</h2><p>Just google it, there are many ways to implement it.</p>]]></content>
      
      
      <categories>
          
          <category> tech </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> hello world </tag>
            
            <tag> blog </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
